\chapter{ترنسفورمر ها و تاریخچه آنها}

از آنجایی که ریشه معماری ترنسفورمر در ترجمه ماشینی و شبکه های عصبی بازگشتی است و معماری های پردازش تصویر بر پایه ترنسفورمر هم به‌شدت از این مفاهیم استفاده می‌کنند، لازم است تا مقدماتی را توضیح دهیم. در این فصل به بررسی RNN ها، شبکه های کاهشی-افزایشی، مکانیزم توجه، معماری ترنسفورمر در حالت خاص ترجمه ماشینی، و در نهایت اقتباس آن‌ها برای عملیات‌های پردازش تصویر می‌پردازیم.

\section{ ترجمه ماشینی}
مسئله ترجمه ماشینی به این صورت تعریف می‌شود که دنباله‌ای از ورودی‌ها $X = \{x_1, x_2, \dots, x_N\}$ در زبان مبدأ داده شده و هدف پیش‌بینی دنباله خروجی $Y = \{y_1, y_2, \dots, y_T\}$ در زبان مقصد است. در اینجا $x_N$ و $ y_T $ به ترتیب به عنوان نشانگر پایان جمله (\texttt{</s>}) در نظر گرفته می‌شوند و $ y_0 $
برابر با شروع جمله (\texttt{<s>}) است. مدل یادگیری ماشین در تلاش برای یافتن عبارت زیر است.

\begin{equation}
\hat{Y} = \arg\max_{Y} P_\theta(Y | X)
\end{equation}

\section{ ترجمه ماشینی بازگشتی}



برای گسترش این فرمول، از قضیه زنجیره‌ای استفاده می‌کنیم تا احتمال شرطی $P_\theta(Y|X)$ را به حاصل‌ضرب احتمالات شرطی تقسیم کنیم:

\begin{align}
	P_{\theta }(Y \mid X) & = P_{\theta }(y_{1:T} \mid X)  \notag \\
	& = P_{\theta }(y_{T} \mid y_{1:T-1}, X) 
	P_{\theta }(y_{T-1} \mid y_{1:T-2}, X)
	...
	P_{\theta }(y_{1} \mid X) \notag \\
	& = \prod_{t=1}^{T} P_\theta(y_t | y_{1:t-1}, X)
\end{align}

فرآیند به این صورت است که مدل، با داشتن ورودی $X$ و دنباله تولید شده تا زمان $t-1$ یعنی $y_{1:t-1}$، احتمال $y_t$ را پیش‌بینی می‌کند. ایده اصلی در مدل‌های ترجمه ماشینی این است که فرایند تولید خروجی $Y$ به صورت ترتیبی انجام می‌شود و در هر مرحله، مدل به خروجی‌های قبلی وابسته است.  برای مدل‌سازی این فرآیند، دو بخش اصلی وجود دارد: کدگذار \lr{(Encoder)} و رمزگشا \lr{(Decoder)}.
\section{شبکه های کدگذار-کدگشا (افزایشی-کاهشی)}
شبکه‌های کدگذار-رمزگشا%
\LTRfootnote{Encoder-Decoder Networks}
یکی از معماری‌های متداول در یادگیری عمیق هستند که به‌طور خاص برای مسائل نیاز1

\subsection{کدگذار (Encoder)}

کدگذار وظیفه دارد اطلاعات توالی ورودی را به یک بازنمایی فشرده تبدیل کند. این فرآیند با استفاده از یک تابع بازگشتی به شکل زیر انجام می‌شود:

$$
h_n = f(x_n, h_{n-1})
$$

که در آن $h_{n-1}$ وضعیت نهان در زمان $ n-1 $ است و $x_n$ ورودی فعلی رمزگذار است.

در نهایت، یک نمایش زمینه‌ای $ c $ با ترکیب بازگشتی بردارهای مخفی $ h $ تولید می‌شود:

$$
c = q(h_{1:N}) = h_N
$$

\subsection{کدگشا (Decoder)}
کدگشا از نمایندگی فشرده تولیدشده توسط کدگذار استفاده کرده و آن را به خروجی مورد نظر نگاشت می‌دهد. این بخش نیز معمولاً از همان نوع شبکه‌های کدگذار استفاده می‌کند، اما با هدف تولید خروجی. فرآیند رمزگشا می‌تواند به شکل مرحله‌به‌مرحله باشد، به‌طوری که هر خروجی تولیدشده به‌عنوان ورودی برای مرحله بعدی استفاده می‌شود.
$$
s_t = g(y_{t-1}, c, s_{t-1})
$$

در اینجا $ g $ تابع تغییر حالت است، $ y_{t-1} $ جاسازی%
\LTRfootnote{Embedding}
آخرین خروجی تخمینی کدگشا است، $s_{t-1}$ آخرین وضعیت نهان کدگشا و $c$ نمایندگی فشرده تولیدشده توسط کدگذار است که یک بازنمایی از کل دنباله ورودی را در بر دارد.

معمولا هر وضعیت نهان $s_t$ به یک عملیات خروجی داده می شود تا در نهایت بدست آید:
$$ P_{\theta}(y_{t} | y_{1:t-1},c) $$

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figs/rnn_without_attn}
	\caption{نمای کلی یک مدل ترجمه ماشینی بر پایه خروجی های بازگشتی. هدف این شبکه آن است که بازنمایی فشرده و خلاصه ای از کل جمله را در آخرین نشانه ورودی
	($\mathbf{</s>}$)
	جای دهد و کدگشا با تکیه بر همین بردار نهایی، به‌صورت بازگشتی خروجی ترجمه شده را تولید کند.
	}
	\label{fig:rnnwithoutattn}
\end{figure}

در شکل \ref{fig:rnnwithoutattn} یک نمای کلی فرایند های گفته شده در ترجمه کلمه‌به‌کلمه ماشینی بازگشتی ارائه شده است.

یکی از محدودیت‌های اصلی معماری کدگذار-رمزگشا \lr{(Encoder-Decoder) } در مدیریت توالی‌های بلند، وابستگی بیش از حد آن به بردار زمینه \lr{(Context Vector)} است. در این معماری، کدگذار ورودی را به یک بردار فشرده واحد تبدیل می‌کند که حاوی اطلاعات معنایی کل توالی است. سپس رمزگشا از این بردار به‌عنوان منبع اصلی اطلاعات برای تولید خروجی استفاده می‌کند. با افزایش طول توالی ورودی، نمایندگی این بردار فشرده ناکارآمد می‌شود و اطلاعات مهم در فرآیند فشرده‌سازی از بین می‌رود. این محدودیت به‌ویژه در مسائلی مانند ترجمه ماشینی، جایی که وابستگی‌های معنایی میان کلمات دورافتاده وجود دارد، به وضوح دیده می‌شود.

مشکل دیگر این معماری، ناتوانی در حفظ ارتباطات بلندمدت میان بخش‌های توالی است. هنگامی که توالی ورودی شامل اطلاعات متنوع و پیچیده باشد، فشرده‌سازی کل محتوا به یک بردار واحد نمی‌تواند تمامی جزئیات را در خود جای دهد. این امر باعث می‌شود که رمزگشا در بازسازی دقیق اطلاعات ورودی، به‌ویژه برای بخش‌های انتهایی یا نقاطی که با اطلاعات اولیه ورودی وابستگی دارند، دچار ضعف شود. به‌عنوان مثال، در ترجمه یک جمله طولانی، مدل ممکن است ساختار دستوری یا معنای کلمات انتهایی را به دلیل از دست رفتن اطلاعات در بردار زمینه نادیده بگیرد.

همچنین، استفاده از یک بردار ثابت به‌عنوان تنها منبع اطلاعاتی رمزگشا باعث می‌شود که توانایی مدل در مدیریت داده‌های پویا و وابستگی‌های متغیر میان بخش‌های توالی کاهش یابد. این محدودیت‌ها در معماری‌های اولیه مانند \lr{Seq2Seq}، که فاقد مکانیزم توجه بودند، بسیار برجسته بودند. برای حل این مشکل، مکانیزم‌های پیشرفته‌تر مانند مکانیزم توجه و مدل‌های ترنسفورمر معرفی شدند که امکان مدل‌سازی ارتباطات محلی و سراسری را به‌طور همزمان فراهم می‌کنند و وابستگی مدل به یک بردار واحد را از بین می‌برند.

\section{مکانیزم توجه}
مکانیزم توجه%
\LTRfootnote{Attention Mechanism}
\cite{bahdanauNeuralMachineTranslation2016} یک تکنیک پیشرفته در معماری‌های یادگیری عمیق است که به مدل‌ها اجازه می‌دهد به بخش‌های مرتبط‌تر ورودی در هر مرحله از تولید خروجی تمرکز کنند. برخلاف معماری‌های سنتی کدگذار-رمزگشا، که کل اطلاعات ورودی را در یک بردار زمینه فشرده می‌کنند، مکانیزم توجه از ماتریس‌های پرسش 
\lr{(Query)}،
 کلید 
 \lr{(Key)}،
  و مقدار \lr{(Value)}  برای محاسبه وزن‌های توجه استفاده می‌کند. این وزن‌ها نشان‌دهنده میزان اهمیت هر بخش ورودی برای تولید یک بخش خاص از خروجی هستند. با استفاده از عملیات Softmax برای نرمال‌سازی، مدل می‌تواند اطلاعات مربوط به قسمت‌های مهم‌تر ورودی را استخراج کرده و به رمزگشا منتقل کند. این ویژگی باعث می‌شود که مکانیزم توجه بتواند ارتباطات بلندمدت و پیچیده میان بخش‌های مختلف توالی را به‌طور موثر مدل‌سازی کند، که این امر در مسائلی مانند ترجمه ماشینی، خلاصه‌سازی متون، و ترمیم تصاویر بسیار کارآمد است.
  
%  \subsection{مکانیزم خود-توجه\protect\LTRfootnote{Self Attention}}

دنباله ورودی
$$\mathbf{X}_{raw} = [x_1, x_2, \dots, x_N]^T \in \mathbb{R}^{N}$$
را در نظر بگیرید که در آن $N$ طول دنباله است و $ x_i $
\textbf{نشانه}
$i$
ام در این ورودی است. ابتدا هر نشانه باید به یک فضای جاسازی نگاشت شود. مثلا فرض میکنیم که صحبت از پردازش زبان است. هر سمبل ورودی باید به یک «جاسازی کلمه‌ای»%
\LTRfootnote{Word Embedding}
نگاشت شود. مثلا با استفاده از یک «ماتریس واژگان»%
\LTRfootnote{Vocabulary Matrix}.
تابعی که این کار را انجام می‌دهد $ Embedding(symbol) $ می‌نامیم. خواهیم داشت:
 $$
 \mathbf{X}_{\text{embed}} = Embedding(\mathbf{X}_{raw}) \in \mathbb{R}^{N \times D_{embed}}
 $$ 

به طوری که $ D_{embed} $ ابعاد فضای جاسازی است.\\
$ D\_{model} $
را ابعاد فضای نهان مدل در نظر بگیرید که در آن تمام نشانه‌ها و داده‌ها در طول پردازش‌ها و تعاملات مختلف مدل (مانند مکانیزم توجه و لایه‌های تمام متصل) نمایش داده می‌شوند. این بعد معمولاً ابعادی است که برای ذخیره‌سازی و پردازش اطلاعات استفاده می‌شود و در مدل‌های ترنسفورمر معمولاً ثابت است. برای مثال، اگر $ D\_{model}$ برابر ۵۱۲ باشد، تمام توکن‌ها، پس از گذر از لایه‌های ورودی و پردازش‌ها، در فضایی با این ابعاد نمایش داده می‌شوند.

حال نگاشت خطی قابل یادگیری $ \mathbf{E} $ را چنان فرض میکنیم که $ X_{embed} $ را از فضای با ابعاد
$\mathbb{R}^{N \times D_{embed}}$
به فضایی با ابعاد ورودی ترنسفورمر (
$\mathbb{R}^{N \times D\_{model}}$
) ببرد. یعنی
$$
	\mathbf{E} \in \mathbb{R}^{D_{embed} \times D\_{model}}
$$

ورودی $ \mathbf{X} $ را پس از اعمال $\mathbf{E} $ روی $ \mathbf{X}_{\text{embed}} $ و سپس اضافه کردن جاسازی موقعیتی می‌سازیم.

\begin{equation}
	\mathbf{X} = \mathbf{X}_{\text{embed}} + PE \in \mathbb{R}^{N \times D\_{model}}
\end{equation}



ماتریس‌های $\mathbf{Q}$ (پرسش)، $\mathbf{K}$ (کلید) و $\mathbf{V}$ (مقدار) با اعمال نگاشت‌های خطی قابل یادگیری به ماتریس ورودی $\mathbf{X}$ محاسبه می‌شوند:
\begin{align*}
	\mathbf{Q} &= \mathbf{X} \mathbf{W}_Q, \quad \mathbf{W}_Q \in \mathbb{R}^{D \times D_K}, \\
	\mathbf{K} &= \mathbf{X} \mathbf{W}_K, \quad \mathbf{W}_K \in \mathbb{R}^{D \times D_K}, \\
	\mathbf{V} &= \mathbf{X} \mathbf{W}_V, \quad \mathbf{W}_V \in \mathbb{R}^{D \times D_V}.
\end{align*}

در اینجا، $\mathbf{W}_Q$, $\mathbf{W}_K$, و $\mathbf{W}_V$ ماتریس‌های وزن قابل یادگیری هستند.

\textbf{محاسبه امتیاز توجه:} 
امتیاز توجه $\alpha_{i,j}$ نشان‌دهنده میزان اهمیت  نشانه%
\LTRfootnote{Token}
شماره $j$ در ارتباط با نشانه $i$ است. به عبارت دیگر، این امتیاز مشخص می‌کند که نشانه $i$ برای تولید خروجی چقدر باید به اطلاعات نشانه $j$ توجه کند. این فرآیند به مدل اجازه می‌دهد تا وابستگی‌های کوتاه‌برد و بلند‌برد را در دنباله ورودی شناسایی کند. با محاسبه این امتیازات، مدل قادر است مفاهیمی مانند همبستگی معنایی، شباهت متنی و حتی ساختارهای گرامری را استخراج کند، که در بسیاری از مسائل پردازش زبان طبیعی (NLP) اهمیت بالایی دارند. یک امتیاز توجه تعریف شده با تابعی مثل $a$ باید به صورت زیر باشد:

$$
a(q,k_n) \in \mathbb{R}
$$

این امتیاز می‌تواند به روش های مختلفی پیاده‌سازی شود. ساده ترین روش برای محاسبه امتیاز توجه، ضرب نقطه‌ای بردار پرسش و بردار کلید است و  به صورت زیر
$$
a(q,k) = q^{T}k
$$
پیاده‌سازی می‌شود. از معروف‌ترین راه‌های به‌دست آوردن چنین امتیازی، استفاده از «توجه ضرب نقطه ای مقیاس‌یافته»%
\LTRfootnote{Scaled Dot Product Attention}
است:
\begin{align*} 
	a{i,j} &= \frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{D_K}},
\end{align*}
که در اینجا $\sqrt{D_K}$ مقیاسی برای جلوگیری از انفجار گرادیان‌ها است.

امتیازات توجه برای تمام نشانه‌ها نرمال‌سازی بیشینه‌نَرم%
\LTRfootnote{Softmax}
شده و سپس ضرب در ماتریس $\mathbf{V}$ می‌شود تا خروجی نهایی تولید شود:


\begin{align}
	\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{D_K}}\right) \mathbf{V} \in \mathbb{R}^{N \times D_K}.
\end{align}

در شکل \ref{fig:attncompgraph} گراف محاسباتی بلوک توجه نشان داده شده است.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figs/attnCompGraph}
	\caption{گراف محاسباتی بلوک توجه}
	\label{fig:attncompgraph}
\end{figure}


\textbf{تفسیر امتیاز توجه:}  
یکی از محدودیت‌های مهم در مدل‌های مبتنی بر بردار زمینه‌ای که پیش از ظهور مکانیزم توجه استفاده می‌شد، این بود که تمامی اطلاعات متن به یک بردار واحد فشرده می‌شد. این فشرده‌سازی باعث از دست رفتن بخشی از اطلاعات مهم، خصوصاً در دنباله‌های طولانی، می‌شد. مکانیزم خودتوجه این محدودیت را با تخصیص وزن‌های متفاوت به نشانه‌های مختلف برطرف کرد. به این ترتیب، هر نشانه می‌تواند  بدون نیاز به وابستگی به یک نقطه مرکزی، اطلاعات موردنیاز خود را به‌طور مستقیم از تمامی دیگر نشانه‌ها استخراج کند‍. 

\textbf{تشابه به توجه در زندگی واقعی:}  
مکانیزم توجه شباهت زیادی به نحوه پردازش اطلاعات در ذهن انسان دارد و مستقیما از مدل ذهنی انسان الهام گرفته است. هنگامی که به یک پاراگراف از متن یا یک تصویر نگاه می‌کنیم، توجه ما به بخش‌های مختلف بسته به اهمیت آنها جلب می‌شود. برای مثال، هنگام خواندن یک جمله، ممکن است بر کلمات کلیدی تمرکز کنیم که برای درک معنای کلی ضروری هستند. مکانیزم توجه در شبکه‌های عصبی نیز به‌طور مشابه عمل می‌کند و وزن بیشتری به اطلاعات مرتبط‌تر اختصاص می‌دهد، در حالی که اطلاعات کم‌اهمیت‌تر را نادیده می‌گیرد. این فرآیند باعث بهبود کارایی مدل ها می‌شود؛ چرا که به مدل توانایی دسترسی با وزن دلخواه به هر بخشی از ورودی را می‌دهد.

\section{مکانیزم خودتوجه در معماری های کاهشی-افزایشی}
اگرچه مکانیزم خودتوجه اغلب به‌عنوان یکی از ویژگی‌های برجسته معماری‌های ترنسفورمر شناخته می‌شود، می‌توان آن را به معماری‌های بازگشتی نیز اضافه کرد تا محدودیت‌های آن‌ها در مدیریت وابستگی‌های بلندمدت کاهش یابد. ایده این است که به جای وابستگی صرف به حالات بازگشتی، رمزگذار و رمزگشا می‌توانند با استفاده از مکانیزم خود-توجه، ارتباطات مستقیم بین نشانه‌های ورودی یا خروجی را مدلسازی کنند.

در هنگام رمزگشایی‌، بجای استفاده از بردار واحد $c$ که می‌توانست موجب از دست رفتن اطلاعات شود،‌ برای هر گام زمانی $t$ یک بردار زمینه $c_t$ محاسبه می‌کنیم. این محاسبه بر پایه مکانیزم خودتوجه است؛ به این صورت که Value ها و Key ها همان وضعیت های نهان  \textbf{کدگذار} (${h_1, h_2, \dots, h_N}$ ) و Query مقدار فعلی حالت پنهان \textbf{کدگشا} است.

کلیدها ($\mathbf{K}$): نمایانگر ویژگی‌های رمزگذاری‌شده توالی ورودی هستند که از حالات پنهان انکدر استخراج می‌شوند: 
$$
K = \{h_1, h_2, \dots, h_N\},
$$
که در آن $ h_n $​ حالت پنهان کدگشا در گام زمانی $\mathbf{n}$ است.

مقادیر ($\mathbf{V}$): شامل همان اطلاعات کلید ها هستند. در اصل مقادیر بردار هایی هستند که امتیاز $\alpha$ توجه بر آن ها اعمال شده و میانگین وزنی مقادیر است که بردار $c_t$ را می‌سازد.
$$
K = \{h_1, h_2, \dots, h_N\},
$$
که در آن $ h_n $​ حالت پنهان کدگشا در گام زمانی $\mathbf{n}$ است.


پرسش‌ها ($\mathbf{Q}$):
نمایانگر حالت پنهان فعلی دیکدر در گام زمانی  $t-1 $ هستند:  
$$
Q = s_{t-1},
$$
که در آن \( s_{t-1} \) حالت پنهان دیکدر در گام زمانی قبلی است.

امتیاز هم‌راستایی بین کوئری $Q$ و هر کلید $ K_n $  با استفاده از تابع امتیازدهی محاسبه می شود که قبلا توضیح داده شد.
$$
a_{t, n} = \text{score}(Q, K_n).
$$

سپس این امتیاز ها بیشینه‌نرم شده و وزن های توجه را می‌سازند.
$$
\alpha_{t, n} = \frac{\exp(a_{t, n})}{\sum_{n'=1}^N \exp(a_{t, n'})}.
$$

بردار زمینه  
بردار زمینه $ c_t $ در گام تولید $t$ به صورت ترکیب وزنی مقادیر $\mathbf{V}$
محاسبه می‌شود:  
$$
c_t = \sum_{n=1}^N \alpha_{t, n} h_n.
$$


 کدگشا مانند قبل بروزرسانی می‌شود.  
$$
s_t = g(y_{t-1}, c_t, s_{t-1}),
$$
که در آن $g$ معمولاً یک \lr{RNN}، یک
LSTM
 یا GRU است.

این مکانیزم به‌صورت پویا تعیین می‌کند که کدام توکن‌های ورودی برای گام فعلی دیکدر مهم‌تر هستند. این ویژگی نه تنها عملکرد بهتری بر روی توالی‌های بلند ارائه می‌دهد، بلکه با نمایش وزن‌های توجه $\alpha_{t,n}$ امکان تفسیر مدل را هم فراهم می‌کند و وابستگی مدل به یک بردار واحد را کم می‌کند. به وضوح این روش نیازمند محاسبات بیشتری است و از پیچیدگی زمانی بیشتری برخوردار است.

\section{مدل‌های ترنسفورمر}

مدل‌های ترنسفورمر که برای اولین بار در \cite{vaswaniAttentionAllYou2023} معرفی شدند، به‌طور کلی از معماری‌های جدیدی بهره می‌برند که از مکانیزم توجه به‌طور کامل استفاده می‌کنند. برخلاف مدل‌های سنتی seq2seq که از RNN یا LSTM برای پردازش توالی‌ها استفاده می‌کنند، ترنسفورمرها نیازی به پردازش ترتیبی ورودی‌ها ندارند و می‌توانند به‌طور همزمان کل توالی ورودی را پردازش کنند. این امر موجب می‌شود که مدل‌های ترنسفورمر برای پردازش داده‌های توالی بلند بسیار کارآمدتر از مدل‌های مبتنی بر RNN یا LSTM باشند.

در این بخش، به بررسی معماری ترنسفورمر و اجزای آن می‌پردازیم، از جمله مکانیزم توجه چندگانه، و معرفی مکانیزمی به نام جاسازی موقعیتی%
\LTRfootnote{Positional Embedding}
برای پردازش توالی‌های ورودی در مدلی که هدف آن موازی سازی است.
\subsection{مکانیزم توجه چندگانه}
در مدل‌های ترنسفورمر، توجه چندگانه به این صورت عمل می‌کند که چندین «سر توجه»%
\LTRfootnote{Attention Head}
به طور موازی اجرا می‌شوند و توزیع های توجه مختلفی محاسبه می‌کنند. به طوری که هر «سر» توجه، امتیاز و وزن های توجه مستقل خودش را محاسبه می‌کند و به ابعاد خاصی از ورودی نگاه می‌کند. این امر به مدل این امکان را می‌دهد که روابط وسیع و گسترده تر و دقیق تری را کشف کند. در نهایت نتیجه همه «سر» ها تجمیع شده و به یک بازنمایی نهایی تبدیل می‌شوند.

ابتدا ورودی به چندین «سر» مختلف تقسیم می‌شود. برای این منظور، بردارهای پرس و جو (\lr{Query})، کلید (\lr{Key})، و مقدار (\lr{Value}) با استفاده از تبدیلات خطیِ مستقل به زیرفضاهای کم‌بُعدتر (\lr{Subspaces}) تصویر می‌شوند. هر سر توجه، این بردارهای تبدیل‌شده را دریافت کرده و با مکانیزم توجه نقطه‌ای-ضربی مقیاس‌شده (\lr{Scaled Dot-Product Attention}) پردازش می‌کند. در این فرآیند، هر سر به بخش‌های متفاوتی از فضای نمایشی ورودی تمرکز می‌کند و الگوهای معنایی یا نحوی خاصی را استخراج می‌نماید.

پس از محاسبه خروجی هر سر، تمامی نتایج به یک بردار واحد الحاق (\lr{Concatenate}) شده و مجدداً از طریق یک تبدیل خطی دیگر به ابعاد مطلوب بازتابانده می‌شوند. این رویکرد دو مزیت عمده دارد: اولاً امکان یادگیری روابط پیچیدهتر با ترکیب اطلاعات از زوایای مختلف فضای نمایشی، و ثانیاً افزایش ظرفیت مدل بدون ایجاد وابستگی بیش از حد به هر یک از سرهای توجه. به این ترتیب، مدل می‌تواند همزمان به انواع متفاوتی از وابستگی‌های بین توکن‌ها (مثلاً روابط دستوری، معنایی یا موقعیتی) توجه کند که این امر قدرت تعمیم‌پذیری ترنسفورمرها را به شدت افزایش می‌دهد. هر «سر توجه» صاحب نگاشت های خطی قابل یادگیری مخصوص خود است.

فرض کنید ورودی (پیش‌پردازش شده) اصلی، ماتریس 
$\mathbf{X} \in \mathbb{R}^{N \times D\_{model}}$
باشد که 
$N$ تعداد توکن‌ها و 
$D_{\text{model}}$ 
بُعد نهفتگی است. برای ایجاد $h$ سر توجه بعد نهفتگی هر «سر» را مشخص کنیم.
$$
D\_{head} = D\_{model} / h
$$
برای هر سر \(i\) (که \(i = 1, \dots, h\))، ماتریس‌های وزن قابل یادگیری زیر را تعریف می‌کنیم:  
\begin{align*}
	\mathbf{W}_i^Q &\in \mathbb{R}^{D\_{model} \times D\_{head}} \quad \text{(نگاشت خطی پرس‌وجو)}, \\
	\mathbf{W}_i^K &\in \mathbb{R}^{D\_{model} \times D\_{head}} \quad \text{(نگاشت خطی کلید)}, \\
	\mathbf{W}_i^V &\in \mathbb{R}^{D\_{model} \times D\_{head}} \quad \text{(نگاشت خطی مقدار)}.
\end{align*}

برای سر \(i\)، ورودی \(\mathbf{X}\) را به پرس‌وجو، کلید و مقدار تبدیل می‌کنیم:  
\begin{align*}
	\mathbf{Q}_i &= \mathbf{X} \mathbf{W}_i^Q \in \mathbb{R}^{N \times D\_{head}}, \\
	\mathbf{K}_i &= \mathbf{X} \mathbf{W}_i^K \in \mathbb{R}^{N \times D\_{head}}, \\
	\mathbf{V}_i &= \mathbf{X} \mathbf{W}_i^V \in \mathbb{R}^{N \times D\_{head}}.
\end{align*}

برای سر \(i\)، امتیازهای توجه و خروجی را محاسبه می‌کنیم:  
$$
\text{head}_i = \text{Attention}(\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i) = \text{softmax}\left(\frac{\mathbf{Q}_i \mathbf{K}_i^T}{\sqrt{D\_{head}}}\right) \mathbf{V}_i \in \mathbb{R}^{N \times D\_{head}}.
$$

در نهایت خروجی همه «سر» ها الحاق می‌شوند، یک نگاشت خطی قابل یاگیری
$\mathbf{W}^O$
اعمال می‌کنیم و داریم:
$$
\text{MultiHead}(\mathbf{X}) = \text{Concat}(\text{head}_1, \dots, \text{head}_H) \mathbf{W}^O \in \mathbb{R}^{N \times D\_{model}},
$$
به طوری که
%$$
%\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V).
%$$
%و
$$
\mathbf{W}^O \in \mathbb{R}^{h \cdot D_{\text{head}} \times D_{\text{model}}}.
$$

در شکل \ref{fig:mhattn1} گرافی برای این عملیات ارائه شده است.

\subsection{جاسازی موقعیتی}

مدل‌های ترنسفورمر به طور ذاتی ترتیبی نیستند. این ویژگی یکی از عوامل اصلی موفقیت آنها محسوب می‌شود. در مقابل، مدل‌های بازگشتی مثل Seq2Seq به طور ذاتی ترتیبی هستند. این بدان معناست که پردازش ورودی‌ها در این مدل‌ها به صورت گام به گام انجام می‌شود و اطلاعات ورودی به صورت ترتیبی وارد مدل می‌شود. سؤال اصلی این است که چگونه می‌توان از این ویژگی ترتیبی مدل‌های RNN برای وظایف پردازش زبان استفاده کرد، هنگامی که معماری ما به یک معماری غیرترتیبی مثل ترنسفومر تغییر می‌یابد؟

تحقیقات نشان داد که می‌توان با استفاده از جاسازی موقعیتی، این مشکل را حل کرد. در این روش، برای مدل ترنسفورمر که به طور ذاتی ترتیبی نیست، اطلاعات موقعیتی به ورودی‌ها اضافه می‌شود. این امر به مدل این امکان را می‌دهد که ترتیب و موقعیت هر نشانه در توالی را در نظر بگیرد، بدون اینکه نیاز به پردازش ترتیبی ورودی‌ها باشد. به عبارت دیگر، \textbf{مدل‌های ترنسفورمر می‌توانند از طریق این جاسازی موقعیتی به صورت غیر ترتیبی عمل کنند، و در عین حال همچنان ویژگی‌های ترتیبی توالی‌ها را درک کنند}.

در این روش، به هر توکن ورودی یک بردار موقعیتی اضافه می‌شود تا اطلاعات مربوط به ترتیب آن در توالی به مدل منتقل شود. این بردارهای موقعیتی می‌توانند به‌طور دلخواه تولید شوند یا حتی میتوانند مانند نگاشت‌های خطی جاسازی، ماتریس هایی قابل‌آموزش باشند. Vaswani و همکاران در تحقیق خود چنین برداری را  به صورت زیر تعریف کرده اند:

\begin{equation}
PE(t, 2i) = \sin\left( \frac{t}{\lambda^{2i/d}} \right), \quad PE(t, 2i+1) = \cos\left( \frac{t}{\lambda^{2i/d}} \right)
\end{equation}
که در آن:
$t$ شماره توکن در توالی است،
$i$ شاخص بعد در بردار موقعیتی است،
و
$d$ ابعاد کل بردار موقعیتی است. 
همچنین $\lambda$ پیشنهاد شده توسط Vaswani و همکاران عدد ۱۰۰۰۰ (ده هزار) است که بسته به طول زمینه%
\LTRfootnote{Context Length}
مورد نظر در دنباله های ورودی، می‌تواند متغیر باشد. این بردارهای موقعیتی به ورودی‌ها اضافه می‌شوند و به مدل این امکان را می‌دهند که ترتیب نشانه‌ها را در توالی در نظر بگیرد.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figs/mhattn1.png}
	\caption{دیاگرام معماری توجه چندگانه. ماتریس $Q$ ماتریس پرسش، ماتریس $K$ ماتریس کلید و ماتریس $V$ مقدار اختصاص یافته به آن کلید است. در هر «سر» توجه همانند قبل از توجه ضرب‌نقطه‌ای استفاده می‌شود.}
	\label{fig:mhattn1}
\end{figure}

%
%\subsection*{ساختار مدل ترنسفورمر}
%مدل ترنسفورمر از دو بخش اصلی تشکیل شده است: رمزگذار و رمزگشا. هر یک از این بخش‌ها شامل چندین لایه هستند که هر کدام شامل مکانیزم‌های توجه چندگانه و یک شبکه عصبی کاملاً متصل (Feed-Forward Neural Network) هستند. در هر لایه، ابتدا توجه چندگانه اعمال می‌شود، سپس نتیجه با یک شبکه عصبی کاملاً متصل (FCN) پردازش می‌شود و در نهایت با استفاده از یک نرمال‌سازی و اعمال یک لایه کانولوشنی، نتیجه نهایی به‌دست می‌آید.


%
%\subsection*{معادلات ترنسفورمر}
%مدل ترنسفورمر به طور خلاصه شامل مراحل زیر است:
%
%1. **لایه توجه چندگانه**:
%\[
%\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
%\]
%که هر سر توجه به صورت زیر عمل می‌کند:
%\[
%\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
%\]
%
%2. **شبکه عصبی کاملاً متصل**:
%\[
%\text{FFN}(x) = \max(0, x W_1 + b_1) W_2 + b_2
%\]
%که در آن \( W_1 \)، \( W_2 \) ماتریس‌های وزن و \( b_1 \)، \( b_2 \) بایاس‌ها هستند.
%
%3. **ساختار انکدر**:
%هر لایه انکدر شامل یک لایه توجه چندگانه و یک شبکه عصبی کاملاً متصل است:
%\[
%\text{EncoderLayer}(x) = \text{FFN}(\text{MultiHeadAttention}(x))
%\]
%این لایه‌ها به‌طور متوالی برای پردازش ورودی‌ها تکرار می‌شوند.
%
%4. **ساختار دیکدر**:
%دیکدر نیز مشابه انکدر است، اما تفاوت اصلی آن این است که ابتدا از توجه ماسک‌شده برای جلوگیری از دسترسی به آینده استفاده می‌کند. معادله مربوط به دیکدر به صورت زیر است:
%\[
%\text{DecoderLayer}(x) = \text{FFN}(\text{MultiHeadAttention}(\text{Masked}(x), \text{EncoderOutput}))
%\]
%در اینجا \( \text{Masked}(x) \) به این معنا است که توجه به ورودی‌ها تنها به توکن‌های قبلی محدود می‌شود.


\section{ترنسفورمر ها در ترمیم تصویر}

خاصیت های مطرح شده و موفقیت شگرف مدل های ترنسفورمر
\cite{radfordLanguageModelsAre2019}
\cite{brownLanguageModelsAre2020}
\cite{openaiGPT4TechnicalReport2024}
در مدل سازی دنباله های متنی طولانی، بدون از دست رفتن وابستگی های بلندمدت، موجب جلب توجه محققان به کاربرد آن‌ها در کابرد های پردازش تصویر نیز شده است.
\cite{liuSwinTransformerHierarchical2021}


یکی از اولین مدل‌های مطرح در این زمینه، مدل ویت (ViT) 
\cite{dosovitskiyImageWorth16x162021}
است که تصویر ورودی را به قطعات کوچک (پچ‌ها) تقسیم کرده و از این قطعه‌ها به عنوان دنباله ورودی به مدل استفاده می‌کند. اما این مدل صرفا یک مدل طبقه بندی است. در فصل بعد روش پیشنهادی ای ارائه می‌دهیم که با الهام گیری از قطعه‌بندی تصویر، یک مدل ترمیم تصویر بر پایه ترنسفورمر بسازیم. 





