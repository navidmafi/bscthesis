@misc{agostinelliMusicLMGeneratingMusic2023,
  title = {{{MusicLM}}: {{Generating Music From Text}}},
  shorttitle = {{{MusicLM}}},
  author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and Sharifi, Matt and Zeghidour, Neil and Frank, Christian},
  year = {2023},
  month = jan,
  number = {arXiv:2301.11325},
  eprint = {2301.11325},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.11325},
  urldate = {2025-02-11},
  abstract = {We introduce MusicLM, a model generating high-fidelity music from text descriptions such as "a calming violin melody backed by a distorted guitar riff". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/2WZBANK8/Agostinelli et al. - 2023 - MusicLM Generating Music From Text.pdf;/home/navid/Zotero/storage/VDYGI2QV/2301.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2025-01-15},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/VCGMKMB3/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/home/navid/Zotero/storage/92NCKYAG/1409.html}
}

@misc{baiDreamDiffusionGeneratingHighQuality2023,
  title = {{{DreamDiffusion}}: {{Generating High-Quality Images}} from {{Brain EEG Signals}}},
  shorttitle = {{{DreamDiffusion}}},
  author = {Bai, Yunpeng and Wang, Xintao and Cao, Yan-pei and Ge, Yixiao and Yuan, Chun and Shan, Ying},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16934},
  eprint = {2306.16934},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.16934},
  urldate = {2025-01-27},
  abstract = {This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pre-trained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs. Overall, the proposed method overcomes the challenges of using EEG signals for image generation, such as noise, limited information, and individual differences, and achieves promising results. Quantitative and qualitative results demonstrate the effectiveness of the proposed method as a significant step towards portable and low-cost ``thoughts-to-image'', with potential applications in neuroscience and computer vision. The code is available here {\textbackslash}url\{https://github.com/bbaaii/DreamDiffusion\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/56L2NCH7/Bai et al. - 2023 - DreamDiffusion Generating High-Quality Images from Brain EEG Signals.pdf;/home/navid/Zotero/storage/BURSCZYC/2306.html}
}

@article{barnesPatchMatchRandomizedCorrespondence2009,
  title = {{{PatchMatch}}: A Randomized Correspondence Algorithm for Structural Image Editing},
  shorttitle = {{{PatchMatch}}},
  author = {Barnes, Connelly and Shechtman, Eli and Finkelstein, Adam and Goldman, Dan B},
  year = {2009},
  month = jul,
  journal = {ACM Trans. Graph.},
  volume = {28},
  number = {3},
  pages = {24:1--24:11},
  issn = {0730-0301},
  doi = {10.1145/1531326.1531330},
  urldate = {2025-01-24},
  abstract = {This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest-neighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However, the cost of computing a field of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x), enabling its use in interactive editing tools. The key insights driving the algorithm are that some good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas. We offer theoretical analysis of the convergence properties of the algorithm, as well as empirical and practical evidence for its high quality and performance. This one simple algorithm forms the basis for a variety of tools -- image retargeting, completion and reshuffling -- that can be used together in the context of a high-level image editing application. Finally, we propose additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods.},
  file = {/home/navid/Zotero/storage/Q4NAMQ7J/Barnes et al. - 2009 - PatchMatch a randomized correspondence algorithm for structural image editing.pdf}
}

@misc{beckXLSTMExtendedLong2024,
  title = {{{xLSTM}}: {{Extended Long Short-Term Memory}}},
  shorttitle = {{{xLSTM}}},
  author = {Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2024},
  month = dec,
  number = {arXiv:2405.04517},
  eprint = {2405.04517},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.04517},
  urldate = {2025-01-25},
  abstract = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/AGKZQAEN/Beck et al. - 2024 - xLSTM Extended Long Short-Term Memory.pdf;/home/navid/Zotero/storage/9UMTVTPI/2405.html}
}

@inproceedings{bertalmioImageInpainting2000,
  title = {Image Inpainting},
  booktitle = {Proceedings of the {{ACM SIGGRAPH Conference}} on {{Computer Graphics}}},
  author = {Bertalm{\'i}o, Marcelo and Sapiro, Guillermo and Caselles, Vicent and Ballester, C.},
  year = {2000},
  month = jan,
  pages = {417--424},
  abstract = {Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.},
  file = {/home/navid/Zotero/storage/I9GYEEYD/Bertalmío et al. - 2000 - Image inpainting.pdf}
}

@misc{borsosAudioLMLanguageModeling2023a,
  title = {{{AudioLM}}: A {{Language Modeling Approach}} to {{Audio Generation}}},
  shorttitle = {{{AudioLM}}},
  author = {Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
  year = {2023},
  month = jul,
  number = {arXiv:2209.03143},
  eprint = {2209.03143},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.03143},
  urldate = {2025-04-25},
  abstract = {We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/4AZC7Q9B/Borsos et al. - 2023 - AudioLM a Language Modeling Approach to Audio Generation.pdf;/home/navid/Zotero/storage/HLSIPGYC/2209.html}
}

@misc{borsosSoundStormEfficientParallel2023,
  title = {{{SoundStorm}}: {{Efficient Parallel Audio Generation}}},
  shorttitle = {{{SoundStorm}}},
  author = {Borsos, Zal{\'a}n and Sharifi, Matt and Vincent, Damien and Kharitonov, Eugene and Zeghidour, Neil and Tagliasacchi, Marco},
  year = {2023},
  month = may,
  number = {arXiv:2305.09636},
  eprint = {2305.09636},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.09636},
  urldate = {2025-01-28},
  abstract = {We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/MHFBTEHT/Borsos et al. - 2023 - SoundStorm Efficient Parallel Audio Generation.pdf;/home/navid/Zotero/storage/XT2SC8EU/2305.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2025-01-25},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/CS77IZUP/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/navid/Zotero/storage/TRT9VJ42/2005.html}
}

@misc{caoLearningSketchTensor2021,
  title = {Learning a {{Sketch Tensor Space}} for {{Image Inpainting}} of {{Man-made Scenes}}},
  author = {Cao, Chenjie and Fu, Yanwei},
  year = {2021},
  month = oct,
  number = {arXiv:2103.15087},
  eprint = {2103.15087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.15087},
  urldate = {2025-01-24},
  abstract = {This paper studies the task of inpainting man-made scenes. It is very challenging due to the difficulty in preserving the visual patterns of images, such as edges, lines, and junctions. Especially, most previous works are failed to restore the object/building structures for images of man-made scenes. To this end, this paper proposes learning a Sketch Tensor (ST) space for inpainting man-made scenes. Such a space is learned to restore the edges, lines, and junctions in images, and thus makes reliable predictions of the holistic image structures. To facilitate the structure refinement, we propose a Multi-scale Sketch Tensor inpainting (MST) network, with a novel encoder-decoder structure. The encoder extracts lines and edges from the input images to project them into an ST space. From this space, the decoder is learned to restore the input images. Extensive experiments validate the efficacy of our model. Furthermore, our model can also achieve competitive performance in inpainting general nature images over the competitors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/LJ2XIPAC/Cao and Fu - 2021 - Learning a Sketch Tensor Space for Image Inpainting of Man-made Scenes.pdf;/home/navid/Zotero/storage/ICNUMTHU/2103.html}
}

@article{chanNontextureInpaintingCurvatureDriven2001,
  title = {Nontexture {{Inpainting}} by {{Curvature-Driven Diffusions}}},
  author = {Chan, Tony F. and Shen, Jianhong},
  year = {2001},
  month = dec,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {12},
  number = {4},
  pages = {436--449},
  issn = {1047-3203},
  doi = {10.1006/jvci.2001.0487},
  urldate = {2025-01-22},
  abstract = {Inpainting is an image interpolation problem, often referring to interpolations over large-scale missing domains. In this paper, guided by the connectivity principle of human visual perception, we introduce a nonlinear PDE inpainting model based upon curvature-driven diffusions for nontexture images. This third-order PDE model improves the second-order total variation inpainting model introduced earlier by Chan and Shen (SIAM J. Appl. Math., in press, 2001). Computational schemes and digital examples are given.},
  file = {/home/navid/Zotero/storage/SYWQALNM/S1047320301904870.html}
}

@misc{chenHINTHighqualityINPainting2024,
  title = {{{HINT}}: {{High-quality INPainting Transformer}} with {{Mask-Aware Encoding}} and {{Enhanced Attention}}},
  shorttitle = {{{HINT}}},
  author = {Chen, Shuang and {Atapour-Abarghouei}, Amir and Shum, Hubert P. H.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.14185},
  eprint = {2402.14185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.14185},
  urldate = {2025-01-14},
  abstract = {Existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in speech recognition, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/XC4QWBHE/Chen et al. - 2024 - HINT High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention.pdf;/home/navid/Zotero/storage/AVCUC66X/2402.html}
}

@misc{chenLearningSeeDark2018,
  title = {Learning to {{See}} in the {{Dark}}},
  author = {Chen, Chen and Chen, Qifeng and Xu, Jia and Koltun, Vladlen},
  year = {2018},
  month = may,
  number = {arXiv:1805.01934},
  eprint = {1805.01934},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.01934},
  urldate = {2025-01-15},
  abstract = {Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can induce blur and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work. The results are shown in the supplementary video at https://youtu.be/qWKUFK7MWvg},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/K97J2X6U/Chen et al. - 2018 - Learning to See in the Dark.pdf;/home/navid/Zotero/storage/ZN9D244J/1805.html}
}

@misc{choElevatingFlowGuidedVideo2024,
  title = {Elevating {{Flow-Guided Video Inpainting}} with {{Reference Generation}}},
  author = {Cho, Suhwan and Oh, Seoung Wug and Lee, Sangyoun and Lee, Joon-Young},
  year = {2024},
  month = dec,
  number = {arXiv:2412.08975},
  eprint = {2412.08975},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08975},
  urldate = {2025-03-06},
  abstract = {Video inpainting (VI) is a challenging task that requires effective propagation of observable content across frames while simultaneously generating new content not present in the original video. In this study, we propose a robust and practical VI framework that leverages a large generative model for reference generation in combination with an advanced pixel propagation algorithm. Powered by a strong generative model, our method not only significantly enhances frame-level quality for object removal but also synthesizes new content in the missing areas based on user-provided text prompts. For pixel propagation, we introduce a one-shot pixel pulling method that effectively avoids error accumulation from repeated sampling while maintaining sub-pixel precision. To evaluate various VI methods in realistic scenarios, we also propose a high-quality VI benchmark, HQVI, comprising carefully generated videos using alpha matte composition. On public benchmarks and the HQVI dataset, our method demonstrates significantly higher visual quality and metric scores compared to existing solutions. Furthermore, it can process high-resolution videos exceeding 2K resolution with ease, underscoring its superiority for real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/Q8AZDQTT/Cho et al. - 2024 - Elevating Flow-Guided Video Inpainting with Reference Generation.pdf;/home/navid/Zotero/storage/83JJYLC7/2412.html;/home/navid/Zotero/storage/F95ISZ4X/2412.html}
}

@misc{cimpoiDescribingTexturesWild2013,
  title = {Describing {{Textures}} in the {{Wild}}},
  author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  year = {2013},
  month = nov,
  number = {arXiv:1311.3618},
  eprint = {1311.3618},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.3618},
  urldate = {2025-01-25},
  abstract = {Patterns and textures are defining characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected in the wild.The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that the describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV, they significantly outperform the state-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/3826RH8U/Cimpoi et al. - 2013 - Describing Textures in the Wild.pdf;/home/navid/Zotero/storage/SQ57KNW4/1311.html}
}

@misc{daiDeepSeekMoEUltimateExpert2024,
  title = {{{DeepSeekMoE}}: {{Towards Ultimate Expert Specialization}} in {{Mixture-of-Experts Language Models}}},
  shorttitle = {{{DeepSeekMoE}}},
  author = {Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, R. X. and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y. and Xie, Zhenda and Li, Y. K. and Huang, Panpan and Luo, Fuli and Ruan, Chong and Sui, Zhifang and Liang, Wenfeng},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06066},
  eprint = {2401.06066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.06066},
  urldate = {2025-01-31},
  abstract = {In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-\$K\$ out of \$N\$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into \$mN\$ ones and activating \$mK\$ from them, allowing for a more flexible combination of activated experts; (2) isolating \$K\_s\$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40\% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5\% (maybe even 18.2\%) of computations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/YXR6SEKN/Dai et al. - 2024 - DeepSeekMoE Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models.pdf;/home/navid/Zotero/storage/G6JW89E4/2401.html}
}

@misc{danbooru2019Portraits,
  title = {Danbooru2019 Portraits: A Large-Scale Anime Head Illustration Dataset},
  author = {Branwen, Gwern and {Anonymous} and Community, Danbooru},
  year = {2019},
  month = mar,
  timestamp = {2019-03-12}
}

@misc{daoFlashAttention2FasterAttention2023,
  title = {{{FlashAttention-2}}: {{Faster Attention}} with {{Better Parallelism}} and {{Work Partitioning}}},
  shorttitle = {{{FlashAttention-2}}},
  author = {Dao, Tri},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08691},
  eprint = {2307.08691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08691},
  urldate = {2025-02-11},
  abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\${\textbackslash}times\$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40{\textbackslash}\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\${\textbackslash}times\$ speedup compared to FlashAttention, reaching 50-73{\textbackslash}\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72{\textbackslash}\% model FLOPs utilization).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/TLZPVW3B/Dao - 2023 - FlashAttention-2 Faster Attention with Better Parallelism and Work Partitioning.pdf;/home/navid/Zotero/storage/945AUZ8W/2307.html}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14135},
  urldate = {2025-02-11},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/MQNDYJDZ/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf;/home/navid/Zotero/storage/TUGGHX93/2205.html}
}

@misc{deepseek-aiDeepSeekR1IncentivizingReasoning2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12948},
  eprint = {2501.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12948},
  urldate = {2025-01-31},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/JZJKG87Z/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf;/home/navid/Zotero/storage/C63AX9UI/2501.html}
}

@misc{deepseek-aiDeepSeekV2StrongEconomical2024,
  title = {{{DeepSeek-V2}}: {{A Strong}}, {{Economical}}, and {{Efficient Mixture-of-Experts Language Model}}},
  shorttitle = {{{DeepSeek-V2}}},
  author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Xu, Hanwei and Yang, Hao and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Chen, Jin and Yuan, Jingyang and Qiu, Junjie and Song, Junxiao and Dong, Kai and Gao, Kaige and Guan, Kang and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Pan, Ruizhe and Xu, Runxin and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Zheng, Size and Wang, T. and Pei, Tian and Yuan, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Liu, Xin and Xie, Xin and Yu, Xingkai and Song, Xinnan and Zhou, Xinyi and Yang, Xinyu and Lu, Xuan and Su, Xuecheng and Wu, Y. and Li, Y. K. and Wei, Y. X. and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Zheng, Yi and Zhang, Yichao and Xiong, Yiliang and Zhao, Yilong and He, Ying and Tang, Ying and Piao, Yishi and Dong, Yixin and Tan, Yixuan and Liu, Yiyuan and Wang, Yongji and Guo, Yongqiang and Zhu, Yuchen and Wang, Yuduan and Zou, Yuheng and Zha, Yukun and Ma, Yunxian and Yan, Yuting and You, Yuxiang and Liu, Yuxuan and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Hao, Zhewen and Shao, Zhihong and Wen, Zhiniu and Xu, Zhipeng and Zhang, Zhongyu and Li, Zhuoshu and Wang, Zihan and Gu, Zihui and Li, Zilin and Xie, Ziwei},
  year = {2024},
  month = jun,
  number = {arXiv:2405.04434},
  eprint = {2405.04434},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.04434},
  urldate = {2025-01-31},
  abstract = {We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5\% of training costs, reduces the KV cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/797I75DF/DeepSeek-AI et al. - 2024 - DeepSeek-V2 A Strong, Economical, and Efficient Mixture-of-Experts Language Model.pdf;/home/navid/Zotero/storage/PX735463/2405.html}
}

@misc{deepseek-aiDeepSeekV3TechnicalReport2024,
  title = {{{DeepSeek-V3 Technical Report}}},
  author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
  year = {2024},
  month = dec,
  number = {arXiv:2412.19437},
  eprint = {2412.19437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.19437},
  urldate = {2025-01-22},
  abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/CFCS44ML/DeepSeek-AI et al. - 2024 - DeepSeek-V3 Technical Report.pdf;/home/navid/Zotero/storage/JSYPBKN3/2412.html}
}

@inproceedings{dengTformerEfficientTransformer2022,
  title = {T-Former: {{An Efficient Transformer}} for {{Image Inpainting}}},
  shorttitle = {T-Former},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author = {Deng, Ye and Hui, Siqi and Zhou, Sanping and Meng, Deyu and Wang, Jinjun},
  year = {2022},
  month = oct,
  eprint = {2305.07239},
  primaryclass = {cs},
  pages = {6559--6568},
  doi = {10.1145/3503161.3548446},
  urldate = {2025-01-15},
  abstract = {Benefiting from powerful convolutional neural networks (CNNs), learning-based image inpainting methods have made significant breakthroughs over the years. However, some nature of CNNs (e.g. local prior, spatially shared parameters) limit the performance in the face of broken images with diverse and complex forms. Recently, a class of attention-based network architectures, called transformer, has shown significant performance on natural language processing fields and high-level vision tasks. Compared with CNNs, attention operators are better at long-range modeling and have dynamic weights, but their computational complexity is quadratic in spatial resolution, and thus less suitable for applications involving higher resolution images, such as image inpainting. In this paper, we design a novel attention linearly related to the resolution according to Taylor expansion. And based on this attention, a network called \$T\$-former is designed for image inpainting. Experiments on several benchmark datasets demonstrate that our proposed method achieves state-of-the-art accuracy while maintaining a relatively low number of parameters and computational complexity. The code can be found at {\textbackslash}href\{https://github.com/dengyecode/T-former\_image\_inpainting\}\{github.com/dengyecode/T-former{\textbackslash}\_image{\textbackslash}\_inpainting\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/6DQ8W7Y3/Deng et al. - 2022 - T-former An Efficient Transformer for Image Inpainting.pdf;/home/navid/Zotero/storage/NCYY463W/2305.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2025-02-18},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/T48SW2A4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf;/home/navid/Zotero/storage/FR9TDJM9/1810.html}
}

@article{domhanSpeedingAutomaticHyperparameter,
  title = {Speeding up {{Automatic Hyperparameter Optimization}} of {{Deep Neural Networks}} by {{Extrapolation}} of {{Learning Curves}}},
  author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  abstract = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
  langid = {english},
  file = {/home/navid/Zotero/storage/DI8443GH/Domhan et al. - Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learni.pdf}
}

@misc{dongImageSuperResolutionUsing2015,
  title = {Image {{Super-Resolution Using Deep Convolutional Networks}}},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  year = {2015},
  month = jul,
  number = {arXiv:1501.00092},
  eprint = {1501.00092},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1501.00092},
  urldate = {2025-02-11},
  abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/home/navid/Zotero/storage/MM2ZH5CH/Dong et al. - 2015 - Image Super-Resolution Using Deep Convolutional Networks.pdf;/home/navid/Zotero/storage/CHVTCE94/1501.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-01-14},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/85Q4YY2X/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/home/navid/Zotero/storage/A8XHW3NI/2010.html}
}

@misc{duPPOCRv2BagTricks2021,
  title = {{{PP-OCRv2}}: {{Bag}} of {{Tricks}} for {{Ultra Lightweight OCR System}}},
  shorttitle = {{{PP-OCRv2}}},
  author = {Du, Yuning and Li, Chenxia and Guo, Ruoyu and Cui, Cheng and Liu, Weiwei and Zhou, Jun and Lu, Bin and Yang, Yehua and Liu, Qiwen and Hu, Xiaoguang and Yu, Dianhai and Ma, Yanjun},
  year = {2021},
  month = oct,
  number = {arXiv:2109.03144},
  eprint = {2109.03144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.03144},
  urldate = {2025-04-25},
  abstract = {Optical Character Recognition (OCR) systems have been widely used in various of application scenarios. Designing an OCR system is still a challenging task. In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR) to balance the accuracy against the efficiency. In order to improve the accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better text detector and a better text recognizer, which include Collaborative Mutual Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the precision of PP-OCRv2 is 7\% higher than PP-OCR under the same inference cost. It is also comparable to the server models of the PP-OCR which uses ResNet series as backbones. All of the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/8BK2IRNU/Du et al. - 2021 - PP-OCRv2 Bag of Tricks for Ultra Lightweight OCR System.pdf;/home/navid/Zotero/storage/R9533XAC/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf;/home/navid/Zotero/storage/AVPK9LIA/2109.html}
}

@article{elharroussImageInpaintingReview2020,
  title = {Image Inpainting: {{A}} Review},
  shorttitle = {Image Inpainting},
  author = {Elharrouss, Omar and Almaadeed, Noor and {Al-Maadeed}, Somaya and Akbari, Younes},
  year = {2020},
  month = apr,
  journal = {Neural Processing Letters},
  volume = {51},
  number = {2},
  eprint = {1909.06399},
  primaryclass = {cs},
  pages = {2007--2028},
  issn = {1370-4621, 1573-773X},
  doi = {10.1007/s11063-019-10163-0},
  urldate = {2025-01-15},
  abstract = {Although image inpainting, or the art of repairing the old and deteriorated images, has been around for many years, it has gained even more popularity because of the recent development in image processing techniques. With the improvement of image processing tools and the flexibility of digital image editing, automatic image inpainting has found important applications in computer vision and has also become an important and challenging topic of research in image processing. This paper is a brief review of the existing image inpainting approaches we first present a global vision on the existing methods for image inpainting. We attempt to collect most of the existing approaches and classify them into three categories, namely, sequential-based, CNN-based and GAN-based methods. In addition, for each category, a list of methods for the different types of distortion on the images is presented. Furthermore, collect a list of the available datasets and discuss these in our paper. This is a contribution for digital image inpainting researchers trying to look for the available datasets because there is a lack of datasets available for image inpainting. As the final step in this overview, we present the results of real evaluations of the three categories of image inpainting methods performed on the datasets used, for the different types of image distortion. In the end, we also present the evaluations metrics and discuss the performance of these methods in terms of these metrics. This overview can be used as a reference for image inpainting researchers, and it can also facilitate the comparison of the methods as well as the datasets used. The main contribution of this paper is the presentation of the three categories of image inpainting methods along with a list of available datasets that the researchers can use to evaluate their proposed methodology against.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/4IU2BHYH/Elharrouss et al. - 2020 - Image inpainting A review.pdf;/home/navid/Zotero/storage/DN3VVSNK/1909.html}
}

@article{elharroussTransformerbasedImageVideo2025,
  title = {Transformer-Based {{Image}} and {{Video Inpainting}}: {{Current Challenges}} and {{Future Directions}}},
  shorttitle = {Transformer-Based {{Image}} and {{Video Inpainting}}},
  author = {Elharrouss, Omar and Damseh, Rafat and Belkacem, Abdelkader Nasreddine and Badidi, Elarbi and Lakas, Abderrahmane},
  year = {2025},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {58},
  number = {4},
  eprint = {2407.00226},
  primaryclass = {cs},
  pages = {124},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-11075-9},
  urldate = {2025-02-15},
  abstract = {Image inpainting is currently a hot topic within the field of computer vision. It offers a viable solution for various applications, including photographic restoration, video editing, and medical imaging. Deep learning advancements, notably convolutional neural networks (CNNs) and generative adversarial networks (GANs), have significantly enhanced the inpainting task with an improved capability to fill missing or damaged regions in an image or video through the incorporation of contextually appropriate details. These advancements have improved other aspects, including efficiency, information preservation, and achieving both realistic textures and structures. Recently, visual transformers have been exploited and offer some improvements to image or video inpainting. The advent of transformer-based architectures, which were initially designed for natural language processing, has also been integrated into computer vision tasks. These methods utilize self-attention mechanisms that excel in capturing long-range dependencies within data; therefore, they are particularly effective for tasks requiring a comprehensive understanding of the global context of an image or video. In this paper, we provide a comprehensive review of the current image or video inpainting approaches, with a specific focus on transformer-based techniques, with the goal to highlight the significant improvements and provide a guideline for new researchers in the field of image or video inpainting using visual transformers. We categorized the transformer-based techniques by their architectural configurations, types of damage, and performance metrics. Furthermore, we present an organized synthesis of the current challenges, and suggest directions for future research in the field of image or video inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/8TM2AZI4/Elharrouss et al. - 2025 - Transformer-based Image and Video Inpainting Current Challenges and Future Directions.pdf;/home/navid/Zotero/storage/6SW23FWK/2407.html}
}

@misc{gaoFlowedgeGuidedVideo2020,
  title = {Flow-Edge {{Guided Video Completion}}},
  author = {Gao, Chen and Saraf, Ayush and Huang, Jia-Bin and Kopf, Johannes},
  year = {2020},
  month = sep,
  number = {arXiv:2009.01835},
  eprint = {2009.01835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.01835},
  urldate = {2025-03-07},
  abstract = {We present a new flow-based video completion algorithm. Previous flow completion methods are often unable to retain the sharpness of motion boundaries. Our method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges. Existing methods propagate colors among local flow connections between adjacent frames. However, not all missing regions in a video can be reached in this way because the motion boundaries form impenetrable barriers. Our method alleviates this problem by introducing non-local flow connections to temporally distant frames, enabling propagating video content over motion boundaries. We validate our approach on the DAVIS dataset. Both visual and quantitative results show that our method compares favorably against the state-of-the-art algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/HUPLF8N8/Gao et al. - 2020 - Flow-edge Guided Video Completion.pdf;/home/navid/Zotero/storage/YJCRL4IC/2009.html}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.2661},
  urldate = {2025-01-24},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/4S48TBWE/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/navid/Zotero/storage/88CEQX8W/1406.html}
}

@misc{guoImageInpaintingConditional2024,
  title = {Image {{Inpainting}} via {{Conditional Texture}} and {{Structure Dual Generation}}},
  author = {Guo, Xiefan and Yang, Hongyu and Huang, Di},
  year = {2024},
  month = apr,
  number = {arXiv:2108.09760},
  eprint = {2108.09760},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.09760},
  urldate = {2025-01-24},
  abstract = {Deep generative approaches have recently made considerable progress in image inpainting by introducing structure priors. Due to the lack of proper interaction with image texture during structure reconstruction, however, current solutions are incompetent in handling the cases with large corruptions, and they generally suffer from distorted results. In this paper, we propose a novel two-stream network for image inpainting, which models the structure-constrained texture synthesis and texture-guided structure reconstruction in a coupled manner so that they better leverage each other for more plausible generation. Furthermore, to enhance the global consistency, a Bi-directional Gated Feature Fusion (Bi-GFF) module is designed to exchange and combine the structure and texture information and a Contextual Feature Aggregation (CFA) module is developed to refine the generated contents by region affinity learning and multi-scale feature aggregation. Qualitative and quantitative experiments on the CelebA, Paris StreetView and Places2 datasets demonstrate the superiority of the proposed method. Our code is available at https://github.com/Xiefan-Guo/CTSDG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/YN546HHX/Guo et al. - 2024 - Image Inpainting via Conditional Texture and Structure Dual Generation.pdf;/home/navid/Zotero/storage/VZBXK9IG/2108.html}
}

@article{haysSceneCompletionUsing2007,
  title = {Scene Completion Using Millions of Photographs},
  author = {Hays, James and Efros, Alexei A.},
  year = {2007},
  month = jul,
  journal = {ACM Trans. Graph.},
  volume = {26},
  number = {3},
  pages = {4--es},
  issn = {0730-0301},
  doi = {10.1145/1276377.1276382},
  urldate = {2025-01-26},
  abstract = {What can you do with a million images? In this paper we present a new image completion algorithm powered by a huge database of photographs gathered from the Web. The algorithm patches up holes in images by finding similar image regions in the database that are not only seamless but also semantically valid. Our chief insight is that while the space of images is effectively infinite, the space of semantically differentiable scenes is actually not that large. For many image completion tasks we are able to find similar scenes which contain image fragments that will convincingly complete the image. Our algorithm is entirely data-driven, requiring no annotations or labelling by the user. Unlike existing image completion methods, our algorithm can generate a diverse set of results for each input image and we allow users to select among them. We demonstrate the superiority of our algorithm over existing image completion approaches.},
  file = {/home/navid/Zotero/storage/Q4SQCKMB/Hays and Efros - 2007 - Scene completion using millions of photographs.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2025-01-14},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/E2VBI6K5/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/navid/Zotero/storage/2KCN6ZM3/1512.html}
}

@misc{heMixtureMillionExperts2024,
  title = {Mixture of {{A Million Experts}}},
  author = {He, Xu Owen},
  year = {2024},
  month = jul,
  number = {arXiv:2407.04153},
  eprint = {2407.04153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.04153},
  urldate = {2025-01-22},
  abstract = {The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/WRXRJY95/He - 2024 - Mixture of A Million Experts.pdf;/home/navid/Zotero/storage/SNBLNLRJ/2407.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Comput.},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2025-01-25},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/navid/Zotero/storage/6FPYJDWY/Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf}
}

@misc{hosenMaskedFaceInpainting2022,
  title = {Masked {{Face Inpainting Through Residual Attention UNet}}},
  author = {Hosen, Md Imran and Islam, Md Baharul},
  year = {2022},
  month = sep,
  number = {arXiv:2209.08850},
  eprint = {2209.08850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.08850},
  urldate = {2025-01-15},
  abstract = {Realistic image restoration with high texture areas such as removing face masks is challenging. The state-of-the-art deep learning-based methods fail to guarantee high-fidelity, cause training instability due to vanishing gradient problems (e.g., weights are updated slightly in initial layers) and spatial information loss. They also depend on intermediary stage such as segmentation meaning require external mask. This paper proposes a blind mask face inpainting method using residual attention UNet to remove the face mask and restore the face with fine details while minimizing the gap with the ground truth face structure. A residual block feeds info to the next layer and directly into the layers about two hops away to solve the gradient vanishing problem. Besides, the attention unit helps the model focus on the relevant mask region, reducing resources and making the model faster. Extensive experiments on the publicly available CelebA dataset show the feasibility and robustness of our proposed model. Code is available at {\textbackslash}url\{https://github.com/mdhosen/Mask-Face-Inpainting-Using-Residual-Attention-Unet\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/navid/Zotero/storage/7GR9FIC8/Hosen and Islam - 2022 - Masked Face Inpainting Through Residual Attention UNet.pdf;/home/navid/Zotero/storage/RHZ56D74/2209.html}
}

@misc{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  number = {arXiv:1704.04861},
  eprint = {1704.04861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.04861},
  urldate = {2025-01-26},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/9M9IP8M4/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf;/home/navid/Zotero/storage/2C6U6Z3Z/1704.html}
}

@misc{huangGameFormerGametheoreticModeling2023,
  title = {{{GameFormer}}: {{Game-theoretic Modeling}} and {{Learning}} of {{Transformer-based Interactive Prediction}} and {{Planning}} for {{Autonomous Driving}}},
  shorttitle = {{{GameFormer}}},
  author = {Huang, Zhiyu and Liu, Haochen and Lv, Chen},
  year = {2023},
  month = aug,
  number = {arXiv:2303.05760},
  eprint = {2303.05760},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.05760},
  urldate = {2025-02-18},
  abstract = {Autonomous vehicles operating in complex real-world environments require accurate predictions of interactive behaviors between traffic participants. This paper tackles the interaction prediction problem by formulating it with hierarchical game theory and proposing the GameFormer model for its implementation. The model incorporates a Transformer encoder, which effectively models the relationships between scene elements, alongside a novel hierarchical Transformer decoder structure. At each decoding level, the decoder utilizes the prediction outcomes from the previous level, in addition to the shared environmental context, to iteratively refine the interaction process. Moreover, we propose a learning process that regulates an agent's behavior at the current level to respond to other agents' behaviors from the preceding level. Through comprehensive experiments on large-scale real-world driving datasets, we demonstrate the state-of-the-art accuracy of our model on the Waymo interaction prediction task. Additionally, we validate the model's capacity to jointly reason about the motion plan of the ego agent and the behaviors of multiple agents in both open-loop and closed-loop planning tests, outperforming various baseline methods. Furthermore, we evaluate the efficacy of our model on the nuPlan planning benchmark, where it achieves leading performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/home/navid/Zotero/storage/H9XLSR54/Huang et al. - 2023 - GameFormer Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Pla.pdf;/home/navid/Zotero/storage/YXLNMBFD/2303.html}
}

@misc{huangIntroVAEIntrospectiveVariational2018,
  title = {{{IntroVAE}}: {{Introspective Variational Autoencoders}} for {{Photographic Image Synthesis}}},
  shorttitle = {{{IntroVAE}}},
  author = {Huang, Huaibo and Li, Zhihang and He, Ran and Sun, Zhenan and Tan, Tieniu},
  year = {2018},
  month = oct,
  number = {arXiv:1807.06358},
  eprint = {1807.06358},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.06358},
  urldate = {2025-01-25},
  abstract = {We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at {\textbackslash}(1024{\textasciicircum}\{2\}{\textbackslash})), which are comparable to or better than the state-of-the-art GANs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/JFJST522/Huang et al. - 2018 - IntroVAE Introspective Variational Autoencoders for Photographic Image Synthesis.pdf;/home/navid/Zotero/storage/2QD6C6U7/1807.html}
}

@article{iizukaGloballyLocallyConsistent2017,
  title = {Globally and Locally Consistent Image Completion},
  author = {Iizuka, Satoshi and {Simo-Serra}, Edgar and Ishikawa, Hiroshi},
  year = {2017},
  month = jul,
  journal = {ACM Trans. Graph.},
  volume = {36},
  number = {4},
  pages = {107:1--107:14},
  issn = {0730-0301},
  doi = {10.1145/3072959.3073659},
  urldate = {2025-01-26},
  abstract = {We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.},
  file = {/home/navid/Zotero/storage/TPJAHHI5/Iizuka et al. - 2017 - Globally and locally consistent image completion.pdf}
}

@article{jamComprehensiveReviewPresent2021,
  title = {A Comprehensive Review of Past and Present Image Inpainting Methods},
  author = {Jam, Jireh and Kendrick, Connah and Walker, Kevin and Drouard, Vincent and Hsu, Jison Gee-Sern and Yap, Moi Hoon},
  year = {2021},
  month = feb,
  journal = {Computer Vision and Image Understanding},
  volume = {203},
  pages = {103147},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2020.103147},
  urldate = {2025-01-23},
  abstract = {Images can be described as visual representations or likeness of something (person or object) which can be reproduced or captured, e.g. a hand drawing, photographic material. However, for images on photographic material, images can have defects at the point of captured, become damaged, or degrade over time. Historically, these were restored by hand to maintain image quality using a process known as inpainting. The advent of the digital age has seen the rapid shift image storage technologies, from hard-copies to digitalised units in a less burdensome manner with the application of digital tools. This paper presents a comprehensive review of image inpainting methods over the past decade and the commonly used performance metrics and datasets. To increase the clarity of our review, we use a hierarchical representation for the past state-of-the-art traditional methods and the present state-of-the-art deep learning methods. For traditional methods, we divide the techniques into five sub-categories, i.e. Exemplar-based texture synthesis, Exemplar-based structure synthesis, Diffusion-based methods, Sparse representation methods and Hybrid methods. Then we review the deep learning methods, i.e. Convolutional Neural Networks and Generative Adversarial Networks. We detail the strengths and weaknesses of each to provide new insights in the field. To address the challenges raised from our findings, we outline some potential future works.},
  keywords = {Convolutional neural network,Generative adversarial networks,Image inpainting,Restoration,Texture synthesis},
  file = {/home/navid/Zotero/storage/8YUBU7Z8/Jam et al. - 2021 - A comprehensive review of past and present image inpainting methods.pdf;/home/navid/Zotero/storage/4C9NEWX8/S1077314220301661.html}
}

@misc{jiangMixtralExperts2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.04088},
  urldate = {2025-01-22},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/Y2FYX75L/Jiang et al. - 2024 - Mixtral of Experts.pdf;/home/navid/Zotero/storage/Q54QYACN/2401.html}
}

@misc{jiaoTinyBERTDistillingBERT2020,
  title = {{{TinyBERT}}: {{Distilling BERT}} for {{Natural Language Understanding}}},
  shorttitle = {{{TinyBERT}}},
  author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  year = {2020},
  month = oct,
  number = {arXiv:1909.10351},
  eprint = {1909.10351},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.10351},
  urldate = {2025-02-18},
  abstract = {Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT. TinyBERT with 4 layers is empirically effective and achieves more than 96.8\% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28\% parameters and about 31\% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/Y8W77M5G/Jiao et al. - 2020 - TinyBERT Distilling BERT for Natural Language Understanding.pdf;/home/navid/Zotero/storage/ZPUVIZ7R/1909.html}
}

@misc{johnsonPerceptualLossesRealTime2016,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  month = mar,
  number = {arXiv:1603.08155},
  eprint = {1603.08155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.08155},
  urldate = {2025-01-26},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/DLEKKD89/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and Super-Resolution.pdf;/home/navid/Zotero/storage/KJZPV3EF/1603.html}
}

@misc{joSCFEGANFaceEditing2019,
  title = {{{SC-FEGAN}}: {{Face Editing Generative Adversarial Network}} with {{User}}'s {{Sketch}} and {{Color}}},
  shorttitle = {{{SC-FEGAN}}},
  author = {Jo, Youngjoo and Park, Jongyoul},
  year = {2019},
  month = feb,
  number = {arXiv:1902.06838},
  eprint = {1902.06838},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.06838},
  urldate = {2025-01-14},
  abstract = {We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/4V6CXBMM/Jo and Park - 2019 - SC-FEGAN Face Editing Generative Adversarial Network with User's Sketch and Color.pdf;/home/navid/Zotero/storage/KVU6ZZ7M/1902.html}
}

@inproceedings{kaplanyanFilteringDistributionsNormals2016,
  title = {Filtering Distributions of Normals for Shading Antialiasing},
  booktitle = {Proceedings of {{High Performance Graphics}}},
  author = {Kaplanyan, A. S. and Hill, S. and Patney, A. and Lefohn, A.},
  year = {2016},
  month = jun,
  series = {{{HPG}} '16},
  pages = {151--162},
  publisher = {Eurographics Association},
  address = {Goslar, DEU},
  urldate = {2025-05-14},
  abstract = {High-frequency illumination effects, such as highly glossy highlights on curved surfaces, are challenging to render in a stable manner. Such features can be much smaller than the area of a pixel and carry a high amount of energy due to high reflectance. These highlights are challenging to render in both offline rendering, where they require many samples and an outliers filter, and in real-time graphics, where they cause a significant amount of aliasing given the small budget of shading samples per pixel. In this paper, we propose a method for filtering the main source of highly glossy highlights in microfacet materials: the Normal Distribution Function (NDF). We provide a practical solution applicable for real-time rendering by employing recent advances in light transport for estimating the filtering region from various effects (such as pixel footprint) directly in the parallel-plane half-vector domain (also known as the slope domain), followed by filtering the NDF over this region. Our real-time method is GPU-friendly, temporally stable, and compatible with deferred shading, normal maps, as well as with filtering methods for normal maps.},
  isbn = {978-3-03868-008-6},
  file = {/home/navid/Zotero/storage/Q2J77XDK/Kaplanyan et al. - 2016 - Filtering distributions of normals for shading antialiasing.pdf}
}

@misc{karrasAliasFreeGenerativeAdversarial2021,
  title = {Alias-{{Free Generative Adversarial Networks}}},
  author = {Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2021},
  month = oct,
  number = {arXiv:2106.12423},
  eprint = {2106.12423},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.12423},
  urldate = {2025-01-24},
  abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/8Z9KGSJR/Karras et al. - 2021 - Alias-Free Generative Adversarial Networks.pdf;/home/navid/Zotero/storage/4R784JHY/2106.html}
}

@misc{karrasAnalyzingImprovingImage2020,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2020},
  month = mar,
  number = {arXiv:1912.04958},
  eprint = {1912.04958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.04958},
  urldate = {2025-01-24},
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/J4USRQCP/Karras et al. - 2020 - Analyzing and Improving the Image Quality of StyleGAN.pdf;/home/navid/Zotero/storage/MYRPYIDA/1912.html}
}

@misc{karrasProgressiveGrowingGANs2018,
  title = {Progressive {{Growing}} of {{GANs}} for {{Improved Quality}}, {{Stability}}, and {{Variation}}},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10196},
  eprint = {1710.10196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.10196},
  urldate = {2025-01-24},
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/DEIQHDB6/Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, Stability, and Variation.pdf;/home/navid/Zotero/storage/F7VX29U7/1710.html}
}

@misc{karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  month = mar,
  number = {arXiv:1812.04948},
  eprint = {1812.04948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.04948},
  urldate = {2025-01-24},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/navid/Zotero/storage/TPXLH7WY/Karras et al. - 2019 - A Style-Based Generator Architecture for Generative Adversarial Networks.pdf;/home/navid/Zotero/storage/V359HC7S/1812.html}
}

@inproceedings{khanfirGraphNeuralNetworks2024,
  title = {Graph {{Neural Networks}} for {{End-to-End Information Extraction}} from {{Handwritten Documents}}},
  booktitle = {2024 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Khanfir, Yessine and Dhiaf, Marwa and Ghodhbani, Emna and Rouhou, Ahmed Cheikh and Kessentini, Yousri},
  year = {2024},
  month = jan,
  pages = {493--501},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACV57701.2024.00056},
  urldate = {2025-04-25},
  abstract = {Automating Information Extraction (IE) from handwritten documents is a challenging task due to the wide variety of handwriting styles, the presence of noise, and the lack of labeled data. In this work, we propose an end-toend encoder-decoder model, that incorporates transformers and Graph Convolutional Networks (GCN), to jointly perform Handwritten Text Recognition (HTR) and Named Entity Recognition (NER). The proposed architecture is mainly composed of two parts: a Sparse Graph Transformer Encoder (SGTE), to capture efficient representations of input text images while controlling the propagation of information through the model. The SGTE is followed by a transformer decoder enhanced with a GCN that combines the outputs of the last SGTE layer and the Multi-Head Attention (MHA) block to reinforce the alignment of visual features to characters and Named Entity (NE) tags, resulting in more robust learned representations. The proposed model shows promising results and achieves state-of-the-art performance on the IAM dataset, and in the ICDAR 2017 Information Extraction competition using the Esposalles database.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-1892-0},
  langid = {english},
  file = {/home/navid/Zotero/storage/IY8G9INJ/Khanfir et al. - 2024 - Graph Neural Networks for End-to-End Information Extraction from Handwritten Documents.pdf}
}

@misc{kimOCRfreeDocumentUnderstanding2022,
  title = {{{OCR-free Document Understanding Transformer}}},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, Jeongyeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year = {2022},
  month = oct,
  number = {arXiv:2111.15664},
  eprint = {2111.15664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.15664},
  urldate = {2025-04-25},
  abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of document; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model and synthetic data are available at https://github.com/clovaai/donut.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/9QRGP8XC/Kim et al. - 2022 - OCR-free Document Understanding Transformer.pdf;/home/navid/Zotero/storage/E9GAFDBF/2111.html}
}

@misc{kimTextureTransformAttention2020,
  title = {Texture {{Transform Attention}} for {{Realistic Image Inpainting}}},
  author = {Kim, Yejin and Cheon, Manri and Lee, Junwoo},
  year = {2020},
  month = dec,
  number = {arXiv:2012.04242},
  eprint = {2012.04242},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.04242},
  urldate = {2025-01-14},
  abstract = {Over the last few years, the performance of inpainting to fill missing regions has shown significant improvements by using deep neural networks. Most of inpainting work create a visually plausible structure and texture, however, due to them often generating a blurry result, final outcomes appear unrealistic and make feel heterogeneity. In order to solve this problem, the existing methods have used a patch based solution with deep neural network, however, these methods also cannot transfer the texture properly. Motivated by these observation, we propose a patch based method. Texture Transform Attention network(TTA-Net) that better produces the missing region inpainting with fine details. The task is a single refinement network and takes the form of U-Net architecture that transfers fine texture features of encoder to coarse semantic features of decoder through skip-connection. Texture Transform Attention is used to create a new reassembled texture map using fine textures and coarse semantics that can efficiently transfer texture information as a result. To stabilize training process, we use a VGG feature layer of ground truth and patch discriminator. We evaluate our model end-to-end with the publicly available datasets CelebA-HQ and Places2 and demonstrate that images of higher quality can be obtained to the existing state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/SP6ZVCF6/Kim et al. - 2020 - Texture Transform Attention for Realistic Image Inpainting.pdf;/home/navid/Zotero/storage/ERBPSQVW/2012.html}
}

@misc{kongHiFiGANGenerativeAdversarial2020,
  title = {{{HiFi-GAN}}: {{Generative Adversarial Networks}} for {{Efficient}} and {{High Fidelity Speech Synthesis}}},
  shorttitle = {{{HiFi-GAN}}},
  author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  year = {2020},
  month = oct,
  number = {arXiv:2010.05646},
  eprint = {2010.05646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.05646},
  urldate = {2025-04-24},
  abstract = {Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/7BBZ8V8J/Kong et al. - 2020 - HiFi-GAN Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis.pdf;/home/navid/Zotero/storage/MA45LD3L/2010.html}
}

@article{krizhevsky2012imagenet,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  journal = {Advances in neural information processing systems},
  volume = {25},
  file = {/home/navid/Zotero/storage/YE7UV2XU/Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional neural networks.pdf}
}

@misc{laubeImageInpaintingHighResolution2018,
  title = {Image {{Inpainting}} for {{High-Resolution Textures}} Using {{CNN Texture Synthesis}}},
  author = {Laube, Pascal and Grunwald, Michael and Franz, Matthias O. and Umlauf, Georg},
  year = {2018},
  month = feb,
  number = {arXiv:1712.03111},
  eprint = {1712.03111},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.03111},
  urldate = {2025-01-15},
  abstract = {Deep neural networks have been successfully applied to problems such as image segmentation, image super-resolution, coloration and image inpainting. In this work we propose the use of convolutional neural networks (CNN) for image inpainting of large regions in high-resolution textures. Due to limited computational resources processing high-resolution images with neural networks is still an open problem. Existing methods separate inpainting of global structure and the transfer of details, which leads to blurry results and loss of global coherence in the detail transfer step. Based on advances in texture synthesis using CNNs we propose patch-based image inpainting by a CNN that is able to optimize for global as well as detail texture statistics. Our method is capable of filling large inpainting regions, oftentimes exceeding the quality of comparable methods for high-resolution images. For reference patch look-up we propose to use the same summary statistics that are used in the inpainting process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/LTPNIA2F/Laube et al. - 2018 - Image Inpainting for High-Resolution Textures using CNN Texture Synthesis.pdf;/home/navid/Zotero/storage/P82USQBK/1712.html}
}

@misc{liEndtoEndFrameworkFlowGuided2022,
  title = {Towards {{An End-to-End Framework}} for {{Flow-Guided Video Inpainting}}},
  author = {Li, Zhen and Lu, Cheng-Ze and Qin, Jianhua and Guo, Chun-Le and Cheng, Ming-Ming},
  year = {2022},
  month = apr,
  number = {arXiv:2204.02663},
  eprint = {2204.02663},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.02663},
  urldate = {2025-03-07},
  abstract = {Optical flow, which captures motion information across frames, is exploited in recent video inpainting methods through propagating pixels along its trajectories. However, the hand-crafted flow-based processes in these methods are applied separately to form the whole inpainting pipeline. Thus, these methods are less efficient and rely heavily on the intermediate results from earlier stages. In this paper, we propose an End-to-End framework for Flow-Guided Video Inpainting (E\${\textasciicircum}2\$FGVI) through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules. The three modules correspond with the three stages of previous flow-based methods but can be jointly optimized, leading to a more efficient and effective inpainting process. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively and shows promising efficiency. The code is available at https://github.com/MCG-NKU/E2FGVI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/navid/Zotero/storage/Q2PEXNVE/Li et al. - 2022 - Towards An End-to-End Framework for Flow-Guided Video Inpainting.pdf;/home/navid/Zotero/storage/LN2Y56H2/2204.html}
}

@misc{liMATMaskAwareTransformer2022,
  title = {{{MAT}}: {{Mask-Aware Transformer}} for {{Large Hole Image Inpainting}}},
  shorttitle = {{{MAT}}},
  author = {Li, Wenbo and Lin, Zhe and Zhou, Kun and Qi, Lu and Wang, Yi and Jia, Jiaya},
  year = {2022},
  month = jun,
  number = {arXiv:2203.15270},
  eprint = {2203.15270},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15270},
  urldate = {2025-01-15},
  abstract = {Recent studies have shown the importance of modeling long-range interactions in the inpainting problem. To achieve this goal, existing approaches exploit either standalone attention techniques or transformers, but usually under a low resolution in consideration of computational cost. In this paper, we present a novel transformer-based model for large hole inpainting, which unifies the merits of transformers and convolutions to efficiently process high-resolution images. We carefully design each component of our framework to guarantee the high fidelity and diversity of recovered images. Specifically, we customize an inpainting-oriented transformer block, where the attention module aggregates non-local information only from partial valid tokens, indicated by a dynamic mask. Extensive experiments demonstrate the state-of-the-art performance of the new model on multiple benchmark datasets. Code is released at https://github.com/fenglinglwb/MAT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/MJDISS8F/Li et al. - 2022 - MAT Mask-Aware Transformer for Large Hole Image Inpainting.pdf;/home/navid/Zotero/storage/QUW5GM76/2203.html}
}

@misc{liPPOCRv3MoreAttempts2022,
  title = {{{PP-OCRv3}}: {{More Attempts}} for the {{Improvement}} of {{Ultra Lightweight OCR System}}},
  shorttitle = {{{PP-OCRv3}}},
  author = {Li, Chenxia and Liu, Weiwei and Guo, Ruoyu and Yin, Xiaoting and Jiang, Kaitao and Du, Yongkun and Du, Yuning and Zhu, Lingfeng and Lai, Baohua and Hu, Xiaoguang and Yu, Dianhai and Ma, Yanjun},
  year = {2022},
  month = jun,
  number = {arXiv:2206.03001},
  eprint = {2206.03001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.03001},
  urldate = {2025-04-25},
  abstract = {Optical character recognition (OCR) technology has been widely used in various scenes, as shown in Figure 1. Designing a practical OCR system is still a meaningful but challenging task. In previous work, considering the efficiency and accuracy, we proposed a practical ultra lightweight OCR system (PP-OCR), and an optimized version PP-OCRv2. In order to further improve the performance of PP-OCRv2, a more robust OCR system PP-OCRv3 is proposed in this paper. PP-OCRv3 upgrades the text detection model and text recognition model in 9 aspects based on PP-OCRv2. For text detector, we introduce a PAN module with large receptive field named LK-PAN, a FPN module with residual attention mechanism named RSE-FPN, and DML distillation strategy. For text recognizer, the base model is replaced from CRNN to SVTR, and we introduce lightweight text recognition network SVTR LCNet, guided training of CTC by attention, data augmentation strategy TextConAug, better pre-trained model by self-supervised TextRotNet, UDML, and UIM to accelerate the model and improve the effect. Experiments on real data show that the hmean of PP-OCRv3 is 5\% higher than PP-OCRv2 under comparable inference speed. All the above mentioned models are open-sourced and the code is available in the GitHub repository PaddleOCR which is powered by PaddlePaddle.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/EL294KAI/Li et al. - 2022 - PP-OCRv3 More Attempts for the Improvement of Ultra Lightweight OCR System.pdf;/home/navid/Zotero/storage/7DYZQYNZ/2206.html}
}

@misc{liRecurrentFeatureReasoning2020,
  title = {Recurrent {{Feature Reasoning}} for {{Image Inpainting}}},
  author = {Li, Jingyuan and Wang, Ning and Zhang, Lefei and Du, Bo and Tao, Dacheng},
  year = {2020},
  month = aug,
  number = {arXiv:2008.03737},
  eprint = {2008.03737},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.03737},
  urldate = {2025-01-24},
  abstract = {Existing inpainting methods have achieved promising performance for recovering regular or small image defects. However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center. In this paper, we devise a Recurrent Feature Reasoning (RFR) network which is mainly constructed by a plug-and-play Recurrent Feature Reasoning module and a Knowledge Consistent Attention (KCA) module. Analogous to how humans solve puzzles (i.e., first solve the easier parts and then use the results as additional information to solve difficult parts), the RFR module recurrently infers the hole boundaries of the convolutional feature maps and then uses them as clues for further inference. The module progressively strengthens the constraints for the hole center and the results become explicit. To capture information from distant places in the feature map for RFR, we further develop KCA and incorporate it in RFR. Empirically, we first compare the proposed RFR-Net with existing backbones, demonstrating that RFR-Net is more efficient (e.g., a 4{\textbackslash}\% SSIM improvement for the same model size). We then place the network in the context of the current state-of-the-art, where it exhibits improved performance. The corresponding source code is available at: https://github.com/jingyuanli001/RFR-Inpainting},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/EA55S3K2/Li et al. - 2020 - Recurrent Feature Reasoning for Image Inpainting.pdf;/home/navid/Zotero/storage/EYW7GS6M/2008.html}
}

@misc{liSTARTSelftaughtReasoner2025,
  title = {{{START}}: {{Self-taught Reasoner}} with {{Tools}}},
  shorttitle = {{{START}}},
  author = {Li, Chengpeng and Xue, Mingfeng and Zhang, Zhenru and Yang, Jiaxi and Zhang, Beichen and Wang, Xiang and Yu, Bowen and Hui, Binyuan and Lin, Junyang and Liu, Dayiheng},
  year = {2025},
  month = mar,
  number = {arXiv:2503.04625},
  eprint = {2503.04625},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.04625},
  urldate = {2025-03-07},
  abstract = {Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6\%, 95.0\%, 66.7\%, 47.1\%, and 47.3\%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/KCN35RPA/Li et al. - 2025 - START Self-taught Reasoner with Tools.pdf;/home/navid/Zotero/storage/ICHQGS5W/2503.html}
}

@misc{liStyleTTS2HumanLevel2023,
  title = {{{StyleTTS}} 2: {{Towards Human-Level Text-to-Speech}} through {{Style Diffusion}} and {{Adversarial Training}} with {{Large Speech Language Models}}},
  shorttitle = {{{StyleTTS}} 2},
  author = {Li, Yinghao Aaron and Han, Cong and Raghavan, Vinay S. and Mischler, Gavin and Mesgarani, Nima},
  year = {2023},
  month = nov,
  number = {arXiv:2306.07691},
  eprint = {2306.07691},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.07691},
  urldate = {2025-04-24},
  abstract = {In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/QKLPXM9Y/Li et al. - 2023 - StyleTTS 2 Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with.pdf;/home/navid/Zotero/storage/XUK27F4E/2306.html}
}

@misc{liuDeepLearningFace2015,
  title = {Deep {{Learning Face Attributes}} in the {{Wild}}},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2015},
  month = sep,
  number = {arXiv:1411.7766},
  eprint = {1411.7766},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1411.7766},
  urldate = {2025-01-24},
  abstract = {Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/GGWWE4QT/Liu et al. - 2015 - Deep Learning Face Attributes in the Wild.pdf;/home/navid/Zotero/storage/ENQSMWDW/1411.html}
}

@misc{liuImageInpaintingIrregular2018,
  title = {Image {{Inpainting}} for {{Irregular Holes Using Partial Convolutions}}},
  author = {Liu, Guilin and Reda, Fitsum A. and Shih, Kevin J. and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
  year = {2018},
  month = dec,
  number = {arXiv:1804.07723},
  eprint = {1804.07723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.07723},
  urldate = {2025-01-24},
  abstract = {Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/7Q3LY5BF/Liu et al. - 2018 - Image Inpainting for Irregular Holes Using Partial Convolutions.pdf;/home/navid/Zotero/storage/ZBEG9HXP/1804.html}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  month = aug,
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14030},
  urldate = {2025-01-25},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/2E6IWHZ9/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer using Shifted Windows.pdf;/home/navid/Zotero/storage/R4LRNXP3/2103.html}
}

@misc{lotfollahiDeepPacketNovel2018,
  title = {Deep {{Packet}}: {{A Novel Approach For Encrypted Traffic Classification Using Deep Learning}}},
  shorttitle = {Deep {{Packet}}},
  author = {Lotfollahi, Mohammad and Zade, Ramin Shirali Hossein and Siavoshani, Mahdi Jafari and Saberian, Mohammdsadegh},
  year = {2018},
  month = jul,
  number = {arXiv:1709.02656},
  eprint = {1709.02656},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.02656},
  urldate = {2025-01-28},
  abstract = {Internet traffic classification has become more important with rapid growth of current Internet network and online applications. There have been numerous studies on this topic which have led to many different approaches. Most of these approaches use predefined features extracted by an expert in order to classify network traffic. In contrast, in this study, we propose a {\textbackslash}emph\{deep learning\} based approach which integrates both feature extraction and classification phases into one system. Our proposed scheme, called "Deep Packet," can handle both {\textbackslash}emph\{traffic characterization\} in which the network traffic is categorized into major classes ({\textbackslash}eg, FTP and P2P) and application identification in which end-user applications ({\textbackslash}eg, BitTorrent and Skype) identification is desired. Contrary to most of the current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN network traffic. After an initial pre-processing phase on data, packets are fed into Deep Packet framework that embeds stacked autoencoder and convolution neural network in order to classify network traffic. Deep packet with CNN as its classification model achieved recall of \$0.98\$ in application identification task and \$0.94\$ in traffic categorization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture},
  file = {/home/navid/Zotero/storage/ZFRYXWYR/Lotfollahi et al. - 2018 - Deep Packet A Novel Approach For Encrypted Traffic Classification Using Deep Learning.pdf;/home/navid/Zotero/storage/3SFTS563/1709.html}
}

@misc{mehtaMobileViTLightweightGeneralpurpose2022,
  title = {{{MobileViT}}: {{Light-weight}}, {{General-purpose}}, and {{Mobile-friendly Vision Transformer}}},
  shorttitle = {{{MobileViT}}},
  author = {Mehta, Sachin and Rastegari, Mohammad},
  year = {2022},
  month = mar,
  number = {arXiv:2110.02178},
  eprint = {2110.02178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.02178},
  urldate = {2025-02-03},
  abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/DC9GARPJ/Mehta and Rastegari - 2022 - MobileViT Light-weight, General-purpose, and Mobile-friendly Vision Transformer.pdf;/home/navid/Zotero/storage/6MMGZQXT/2110.html}
}

@misc{nazeriEdgeConnectGenerativeImage2019,
  title = {{{EdgeConnect}}: {{Generative Image Inpainting}} with {{Adversarial Edge Learning}}},
  shorttitle = {{{EdgeConnect}}},
  author = {Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal Z. and Ebrahimi, Mehran},
  year = {2019},
  month = jan,
  number = {arXiv:1901.00212},
  eprint = {1901.00212},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.00212},
  urldate = {2025-01-24},
  abstract = {Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: https://github.com/knazeri/edge-connect},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/FP2ENJVQ/Nazeri et al. - 2019 - EdgeConnect Generative Image Inpainting with Adversarial Edge Learning.pdf;/home/navid/Zotero/storage/M7SN7NXX/1901.html}
}

@misc{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2024},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2025-01-24},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/navid/Zotero/storage/8PJL92MC/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf;/home/navid/Zotero/storage/BTDIYR9L/2303.html}
}

@misc{parmarImageTransformer2018,
  title = {Image {{Transformer}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  month = jun,
  number = {arXiv:1802.05751},
  eprint = {1802.05751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.05751},
  urldate = {2025-01-14},
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/2IB48WKS/Parmar et al. - 2018 - Image Transformer.pdf;/home/navid/Zotero/storage/J6R4KNUH/1802.html}
}

@misc{pathakContextEncodersFeature2016,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year = {2016},
  month = apr,
  journal = {arXiv.org},
  urldate = {2025-01-24},
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  howpublished = {https://arxiv.org/abs/1604.07379v2},
  langid = {english},
  file = {/home/navid/Zotero/storage/IC6IEZGJ/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf}
}

@misc{pengGeneratingDiverseStructure2021,
  title = {Generating {{Diverse Structure}} for {{Image Inpainting With Hierarchical VQ-VAE}}},
  author = {Peng, Jialun and Liu, Dong and Xu, Songcen and Li, Houqiang},
  year = {2021},
  month = mar,
  number = {arXiv:2103.10022},
  eprint = {2103.10022},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.10022},
  urldate = {2025-01-24},
  abstract = {Given an incomplete image without additional constraint, image inpainting natively allows for multiple solutions as long as they appear plausible. Recently, multiplesolution inpainting methods have been proposed and shown the potential of generating diverse results. However, these methods have difficulty in ensuring the quality of each solution, e.g. they produce distorted structure and/or blurry texture. We propose a two-stage model for diverse inpainting, where the first stage generates multiple coarse results each of which has a different structure, and the second stage refines each coarse result separately by augmenting texture. The proposed model is inspired by the hierarchical vector quantized variational auto-encoder (VQ-VAE), whose hierarchical architecture isentangles structural and textural information. In addition, the vector quantization in VQVAE enables autoregressive modeling of the discrete distribution over the structural information. Sampling from the distribution can easily generate diverse and high-quality structures, making up the first stage of our model. In the second stage, we propose a structural attention module inside the texture generation network, where the module utilizes the structural information to capture distant correlations. We further reuse the VQ-VAE to calculate two feature losses, which help improve structure coherence and texture realism, respectively. Experimental results on CelebA-HQ, Places2, and ImageNet datasets show that our method not only enhances the diversity of the inpainting solutions but also improves the visual quality of the generated multiple images. Code and models are available at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/PS6XTZ49/Peng et al. - 2021 - Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE.pdf;/home/navid/Zotero/storage/W8E3AS5N/2103.html}
}

@misc{qinOpenVoiceVersatileInstant2024,
  title = {{{OpenVoice}}: {{Versatile Instant Voice Cloning}}},
  shorttitle = {{{OpenVoice}}},
  author = {Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  year = {2024},
  month = aug,
  number = {arXiv:2312.01479},
  eprint = {2312.01479},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01479},
  urldate = {2025-04-24},
  abstract = {We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language. OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code and trained model publicly accessible. We also provide qualitative results in our demo website. OpenVoice has been used by more than 2M users worldwide as the voice engine of MyShell.ai},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/ADTMPFXE/Qin et al. - 2024 - OpenVoice Versatile Instant Voice Cloning.pdf;/home/navid/Zotero/storage/BDLMJND8/2312.html}
}

@misc{quanDeepLearningbasedImage2024,
  title = {Deep {{Learning-based Image}} and {{Video Inpainting}}: {{A Survey}}},
  shorttitle = {Deep {{Learning-based Image}} and {{Video Inpainting}}},
  author = {Quan, Weize and Chen, Jiaxi and Liu, Yanli and Yan, Dong-Ming and Wonka, Peter},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03395},
  eprint = {2401.03395},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.03395},
  urldate = {2025-01-22},
  abstract = {Image and video inpainting is a classic problem in computer vision and computer graphics, aiming to fill in the plausible and realistic content in the missing areas of images and videos. With the advance of deep learning, this problem has achieved significant progress recently. The goal of this paper is to comprehensively review the deep learning-based methods for image and video inpainting. Specifically, we sort existing methods into different categories from the perspective of their high-level inpainting pipeline, present different deep learning architectures, including CNN, VAE, GAN, diffusion models, etc., and summarize techniques for module design. We review the training objectives and the common benchmark datasets. We present evaluation metrics for low-level pixel and high-level perceptional similarity, conduct a performance evaluation, and discuss the strengths and weaknesses of representative inpainting methods. We also discuss related real-world applications. Finally, we discuss open challenges and suggest potential future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/DEEWHJYR/Quan et al. - 2024 - Deep Learning-based Image and Video Inpainting A Survey.pdf;/home/navid/Zotero/storage/M3JTSRIQ/2401.html}
}

@article{radfordLanguageModelsAre2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  file = {/home/navid/Zotero/storage/BFNIGMVV/Radford et al. - 2019 - Language models are unsupervised multitask learners.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-01-24},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/YR64Y38R/Radford et al. - 2021 - Learning Transferable Visual Models From Natural Language Supervision.pdf;/home/navid/Zotero/storage/LMKU3IZV/2103.html}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12092},
  eprint = {2102.12092},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.12092},
  urldate = {2025-01-24},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/ADKAJ34R/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;/home/navid/Zotero/storage/5LVKFA47/2102.html}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02640},
  urldate = {2025-02-16},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/W2WPWHC6/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf;/home/navid/Zotero/storage/64ES59NR/1506.html}
}

@misc{renFastSpeech2Fast2020,
  title = {{{FastSpeech}} 2: {{Fast}} and {{High-Quality End-to-End Text-to-Speech}}},
  shorttitle = {{{FastSpeech}} 2},
  author = {Ren, Yi and Hu, Chenxu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  year = {2020},
  month = jun,
  number = {arXiv:2006.04558},
  eprint = {2006.04558},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.04558},
  urldate = {2025-04-24},
  abstract = {Advanced text-to-speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs during training and use predicted values during inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of full end-to-end training and even faster inference than FastSpeech. Experimental results show that 1) FastSpeech 2 and 2s outperform FastSpeech in voice quality with much simplified training pipeline and reduced training time; 2) FastSpeech 2 and 2s can match the voice quality of autoregressive models while enjoying much faster inference speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/58P7LB9S/Ren et al. - 2020 - FastSpeech 2 Fast and High-Quality End-to-End Text-to-Speech.pdf;/home/navid/Zotero/storage/EUX2H7FK/2006.html}
}

@inproceedings{rojasReviewImageInpainting2020,
  title = {A {{Review}} on {{Image Inpainting Techniques}} and {{Datasets}}},
  booktitle = {2020 33rd {{SIBGRAPI Conference}} on {{Graphics}}, {{Patterns}} and {{Images}} ({{SIBGRAPI}})},
  author = {Rojas, David Josu{\'e} Barrientos and Fernandes, Bruno Jos{\'e} Torres and Fernandes, Sergio Murilo Maciel},
  year = {2020},
  month = nov,
  pages = {240--247},
  issn = {2377-5416},
  doi = {10.1109/SIBGRAPI51738.2020.00040},
  urldate = {2025-01-15},
  abstract = {Image inpainting is a process that allows filling in target regions with alternative contents by estimating the suitable information from auxiliary data, either from surrounding areas or external sources. Digital image inpainting techniques are classified in traditional techniques and Deep Learning techniques. Traditional techniques are able to produce accurate high-quality results when the missing areas are small, however none of them are able to generate novel objects not found in the source image neither to produce semantically consistent results. Deep Learning techniques have greatly improved the quality on image inpainting delivering promising results by generating semantic hole filling and novel objects not found in the original image. However, there is still a lot of room for improvement, specially on arbitrary image sizes, arbitrary masks, high resolution texture synthesis, reduction of computation resources and reduction of training time. This work classifies and orders chronologically the most prominent techniques, providing an overall explanation on its operation. It presents, as well, the most used datasets and evaluation metrics across all the works reviewed.},
  keywords = {Convolution,convolution based,dataset,deep learning,Deep learning,diffusion based,Image reconstruction,inpainting,Kernel,Mathematical model,Optimization,patch based,reconstruction,TV}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2025-01-22},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/9CVGAWS7/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf;/home/navid/Zotero/storage/W9C6VFAE/2112.html}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2025-01-14},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/ZHQ444VS/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf;/home/navid/Zotero/storage/5N7MGXWE/1505.html}
}

@misc{russakovskyImageNetLargeScale2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2015},
  month = jan,
  number = {arXiv:1409.0575},
  eprint = {1409.0575},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.0575},
  urldate = {2025-01-24},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/9RX6668G/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf;/home/navid/Zotero/storage/425Z9ALH/1409.html}
}

@misc{senINRVContinuousRepresentation2023,
  title = {{{INR-V}}: {{A Continuous Representation Space}} for {{Video-based Generative Tasks}}},
  shorttitle = {{{INR-V}}},
  author = {Sen, Bipasha and Agarwal, Aditya and Namboodiri, Vinay P. and Jawahar, C. V.},
  year = {2023},
  month = apr,
  number = {arXiv:2210.16579},
  eprint = {2210.16579},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.16579},
  urldate = {2025-03-06},
  abstract = {Generating videos is a complex task that is accomplished by generating a set of temporally coherent images frame-by-frame. This limits the expressivity of videos to only image-based operations on the individual video frames needing network designs to obtain temporally coherent trajectories in the underlying image space. We propose INR-V, a video representation network that learns a continuous space for video-based generative tasks. INR-V parameterizes videos using implicit neural representations (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel location of the video. The INR is predicted using a meta-network which is a hypernetwork trained on neural representations of multiple video instances. Later, the meta-network can be sampled to generate diverse novel videos enabling many downstream video-based generative tasks. Interestingly, we find that conditional regularization and progressive weight initialization play a crucial role in obtaining INR-V. The representation space learned by INR-V is more expressive than an image space showcasing many interesting properties not possible with the existing works. For instance, INR-V can smoothly interpolate intermediate videos between known video instances (such as intermediate identities, expressions, and poses in face videos). It can also in-paint missing portions in videos to recover temporally coherent full videos. In this work, we evaluate the space learned by INR-V on diverse generative tasks such as video interpolation, novel video generation, video inversion, and video inpainting against the existing baselines. INR-V significantly outperforms the baselines on several of these demonstrated tasks, clearly showcasing the potential of the proposed representation space.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/NSJZH64M/Sen et al. - 2023 - INR-V A Continuous Representation Space for Video-based Generative Tasks.pdf;/home/navid/Zotero/storage/MW2PPZ9E/2210.html}
}

@misc{seyfiogluDiffuseChooseEnriching2024,
  title = {Diffuse to {{Choose}}: {{Enriching Image Conditioned Inpainting}} in {{Latent Diffusion Models}} for {{Virtual Try-All}}},
  shorttitle = {Diffuse to {{Choose}}},
  author = {Seyfioglu, Mehmet Saygin and Bouyarmane, Karim and Kumar, Suren and Tavanaei, Amir and Tutar, Ismail B.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13795},
  eprint = {2401.13795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13795},
  urldate = {2025-01-22},
  abstract = {As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as "Virtual Try-All"-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present "Diffuse to Choose," a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/3YPVUZFC/Seyfioglu et al. - 2024 - Diffuse to Choose Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try.pdf;/home/navid/Zotero/storage/JQIT2MYY/2401.html}
}

@misc{shaoDeepSeekMathPushingLimits2024,
  title = {{{DeepSeekMath}}: {{Pushing}} the {{Limits}} of {{Mathematical Reasoning}} in {{Open Language Models}}},
  shorttitle = {{{DeepSeekMath}}},
  author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
  year = {2024},
  month = apr,
  number = {arXiv:2402.03300},
  eprint = {2402.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03300},
  urldate = {2025-01-31},
  abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/RIE856EA/Shao et al. - 2024 - DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf;/home/navid/Zotero/storage/6QB5UFHW/2402.html}
}

@misc{shiConvolutionalLSTMNetwork2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  year = {2015},
  month = sep,
  number = {arXiv:1506.04214},
  eprint = {1506.04214},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.04214},
  urldate = {2025-01-26},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/S8K559BK/Shi et al. - 2015 - Convolutional LSTM Network A Machine Learning Approach for Precipitation Nowcasting.pdf;/home/navid/Zotero/storage/JNPKTRX8/1506.html}
}

@inproceedings{tokuyoshiImprovedGeometricSpecular2019,
  title = {Improved Geometric Specular Antialiasing},
  booktitle = {Proceedings of the {{ACM SIGGRAPH Symposium}} on {{Interactive 3D Graphics}} and {{Games}}},
  author = {Tokuyoshi, Yusuke and Kaplanyan, Anton S.},
  year = {2019},
  month = may,
  series = {{{I3D}} '19},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3306131.3317026},
  urldate = {2025-05-14},
  abstract = {Shading filtering proposed by Kaplanyan et al. [2016] is a simple solution for specular aliasing. It filters a distribution of microfacet normals in the domain of microfacet slopes by estimating the filtering kernel using derivatives of a halfway vector between incident and outdoing directions. However, for real-time rendering, this approach can produce noticeable artifacts because of an estimation error of derivatives. For forward rendering, this estimation error is increased significantly at grazing angles and near edges. The present work improves the quality of the original technique, while decreasing the complexity of the code at the same time. To reduce the error, we introduce a more efficient kernel bandwidth that takes the angle of the halfvector into account. In addition, we optimize the calculation of an isotropic filter kernel used for deferred rendering by applying the proposed kernel bandwidth. As our implementation is simpler than the original method, it is easier to integrate in time-sensitive applications, such as game engines, while at the same time improving the filtering quality.},
  isbn = {978-1-4503-6310-5},
  file = {/home/navid/Zotero/storage/SQWVUVXY/Tokuyoshi and Kaplanyan - 2019 - Improved geometric specular antialiasing.pdf}
}

@misc{touvronAugmentingConvolutionalNetworks2021,
  title = {Augmenting {{Convolutional}} Networks with Attention-Based Aggregation},
  author = {Touvron, Hugo and Cord, Matthieu and {El-Nouby}, Alaaeldin and Bojanowski, Piotr and Joulin, Armand and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = dec,
  number = {arXiv:2112.13692},
  eprint = {2112.13692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.13692},
  urldate = {2025-02-12},
  abstract = {We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/XFD6EYTT/Touvron et al. - 2021 - Augmenting Convolutional networks with attention-based aggregation.pdf;/home/navid/Zotero/storage/BGZBKR36/2112.html}
}

@misc{touvronCotraining$2^L$Submodels2022,
  title = {Co-Training \$2{\textasciicircum}{{L}}\$ {{Submodels}} for {{Visual Recognition}}},
  author = {Touvron, Hugo and Cord, Matthieu and Oquab, Maxime and Bojanowski, Piotr and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04884},
  eprint = {2212.04884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.04884},
  urldate = {2025-02-12},
  abstract = {We introduce submodel co-training, a regularization method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, ``submodels'', with stochastic depth: we activate only a subset of the layers. Each network serves as a soft teacher to the other, by providing a loss that complements the regular loss provided by the one-hot label. Our approach, dubbed cosub, uses a single set of weights, and does not involve a pre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is effective to train backbones for recognition tasks such as image classification and semantic segmentation. Our approach is compatible with multiple architectures, including RegNet, ViT, PiT, XCiT, Swin and ConvNext. Our training strategy improves their results in comparable settings. For instance, a ViT-B pretrained with cosub on ImageNet-21k obtains 87.4\% top-1 acc. @448 on ImageNet-val.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/L4H2BWGZ/Touvron et al. - 2022 - Co-training $2^L$ Submodels for Visual Recognition.pdf;/home/navid/Zotero/storage/YSE4XLZP/2212.html}
}

@misc{touvronDeiTIIIRevenge2022,
  title = {{{DeiT III}}: {{Revenge}} of the {{ViT}}},
  shorttitle = {{{DeiT III}}},
  author = {Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07118},
  eprint = {2204.07118},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.07118},
  urldate = {2025-02-12},
  abstract = {A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations, closer to the practice in self-supervised learning. Our evaluations on Image classification (ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better baselines for recent self-supervised approaches demonstrated on ViT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/IENDD3EY/Touvron et al. - 2022 - DeiT III Revenge of the ViT.pdf;/home/navid/Zotero/storage/EF4YV5XW/2204.html}
}

@misc{touvronGoingDeeperImage2021,
  title = {Going Deeper with {{Image Transformers}}},
  author = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = apr,
  number = {arXiv:2103.17239},
  eprint = {2103.17239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.17239},
  urldate = {2025-02-12},
  abstract = {Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of image transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two transformers architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for instance we obtain 86.5\% top-1 accuracy on Imagenet when training with no external data, we thus attain the current SOTA with less FLOPs and parameters. Moreover, our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/2GRLNW3I/Touvron et al. - 2021 - Going deeper with Image Transformers.pdf;/home/navid/Zotero/storage/U89HILGX/2103.html}
}

@misc{touvronResMLPFeedforwardNetworks2021,
  title = {{{ResMLP}}: {{Feedforward}} Networks for Image Classification with Data-Efficient Training},
  shorttitle = {{{ResMLP}}},
  author = {Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and {El-Nouby}, Alaaeldin and Grave, Edouard and Izacard, Gautier and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jun,
  number = {arXiv:2105.03404},
  eprint = {2105.03404},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.03404},
  urldate = {2025-02-12},
  abstract = {We present ResMLP, an architecture built entirely upon multi-layer perceptrons for image classification. It is a simple residual network that alternates (i) a linear layer in which image patches interact, independently and identically across channels, and (ii) a two-layer feed-forward network in which channels interact independently per patch. When trained with a modern training strategy using heavy data-augmentation and optionally distillation, it attains surprisingly good accuracy/complexity trade-offs on ImageNet. We also train ResMLP models in a self-supervised setup, to further remove priors from employing a labelled dataset. Finally, by adapting our model to machine translation we achieve surprisingly good results. We share pre-trained models and our code based on the Timm library.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/QVDG4D9X/Touvron et al. - 2021 - ResMLP Feedforward networks for image classification with data-efficient training.pdf;/home/navid/Zotero/storage/8JDPZCWP/2105.html}
}

@misc{touvronThreeThingsEveryone2022,
  title = {Three Things Everyone Should Know about {{Vision Transformers}}},
  author = {Touvron, Hugo and Cord, Matthieu and {El-Nouby}, Alaaeldin and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  year = {2022},
  month = mar,
  number = {arXiv:2203.09795},
  eprint = {2203.09795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.09795},
  urldate = {2025-02-12},
  abstract = {After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/XUNH59FB/Touvron et al. - 2022 - Three things everyone should know about Vision Transformers.pdf;/home/navid/Zotero/storage/33NIT6ZW/2203.html}
}

@misc{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jan,
  number = {arXiv:2012.12877},
  eprint = {2012.12877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.12877},
  urldate = {2025-01-30},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/BL5SM6TT/Touvron et al. - 2021 - Training data-efficient image transformers & distillation through attention.pdf;/home/navid/Zotero/storage/7JNTRFQS/2012.html;/home/navid/Zotero/storage/R3PSICNG/2012.html}
}

@misc{valinLPCNetImprovingNeural2019,
  title = {{{LPCNet}}: {{Improving Neural Speech Synthesis Through Linear Prediction}}},
  shorttitle = {{{LPCNet}}},
  author = {Valin, Jean-Marc and Skoglund, Jan},
  year = {2019},
  month = feb,
  number = {arXiv:1810.11846},
  eprint = {1810.11846},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.11846},
  urldate = {2025-04-24},
  abstract = {Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/N2S7EJ5Y/Valin and Skoglund - 2019 - LPCNet Improving Neural Speech Synthesis Through Linear Prediction.pdf;/home/navid/Zotero/storage/DHAG2KBP/1810.html}
}

@misc{valinVeryLowComplexity2024,
  title = {Very {{Low Complexity Speech Synthesis Using Framewise Autoregressive GAN}} ({{FARGAN}}) with {{Pitch Prediction}}},
  author = {Valin, Jean-Marc and Mustafa, Ahmed and B{\"u}the, Jan},
  year = {2024},
  month = aug,
  number = {arXiv:2405.21069},
  eprint = {2405.21069},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.21069},
  urldate = {2025-04-24},
  abstract = {Neural vocoders are now being used in a wide range of speech processing applications. In many of those applications, the vocoder can be the most complex component, so finding lower complexity algorithms can lead to significant practical benefits. In this work, we propose FARGAN, an autoregressive vocoder that takes advantage of long-term pitch prediction to synthesize high-quality speech in small subframes, without the need for teacher-forcing. Experimental results show that the proposed 600{\textasciitilde}MFLOPS FARGAN vocoder can achieve both higher quality and lower complexity than existing low-complexity vocoders. The quality even matches that of existing higher-complexity vocoders.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/navid/Zotero/storage/ZGDAIJNL/Valin et al. - 2024 - Very Low Complexity Speech Synthesis Using Framewise Autoregressive GAN (FARGAN) with Pitch Predicti.pdf;/home/navid/Zotero/storage/2Q7S8REG/2405.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-01-14},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/LY9TFKBS/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/home/navid/Zotero/storage/X6H4X5VN/1706.html}
}

@misc{wanBringingOldPhotos2020,
  title = {Bringing {{Old Photos Back}} to {{Life}}},
  author = {Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},
  year = {2020},
  month = apr,
  number = {arXiv:2004.09484},
  eprint = {2004.09484},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09484},
  urldate = {2025-01-15},
  abstract = {We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Specifically, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting to the structured defects, such as scratches and dust spots, and a local branch targeting to the unstructured defects, such as noises and blurriness. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. The proposed method outperforms state-of-the-art methods in terms of visual quality for old photos restoration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/navid/Zotero/storage/UIPNWLJ4/Wan et al. - 2020 - Bringing Old Photos Back to Life.pdf;/home/navid/Zotero/storage/PHAQXDRR/2004.html}
}

@misc{wangTooLargeData2023,
  title = {Too {{Large}}; {{Data Reduction}} for {{Vision-Language Pre-Training}}},
  author = {Wang, Alex Jinpeng and Lin, Kevin Qinghong and Zhang, David Junhao and Lei, Stan Weixian and Shou, Mike Zheng},
  year = {2023},
  month = aug,
  number = {arXiv:2305.20087},
  eprint = {2305.20087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.20087},
  urldate = {2025-02-18},
  abstract = {This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR, which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M (\${\textbackslash}sim\$24{\textbackslash}\%) and noisy YFCC15M from 15M to 2.5M (\${\textbackslash}sim\$16.7{\textbackslash}\%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset. The code will be made available at {\textbackslash}url\{https://github.com/showlab/datacentric.vlp\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/D9HXKUT8/Wang et al. - 2023 - Too Large\; Data Reduction for Vision-Language Pre-Training.pdf;/home/navid/Zotero/storage/PBJ8HICZ/2305.html}
}

@misc{wangYOLOv10RealTimeEndtoEnd2024,
  title = {{{YOLOv10}}: {{Real-Time End-to-End Object Detection}}},
  shorttitle = {{{YOLOv10}}},
  author = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  year = {2024},
  month = oct,
  number = {arXiv:2405.14458},
  eprint = {2405.14458},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.14458},
  urldate = {2025-02-16},
  abstract = {Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8\${\textbackslash}times\$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8\${\textbackslash}times\$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46{\textbackslash}\% less latency and 25{\textbackslash}\% fewer parameters for the same performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/7R496R2H/Wang et al. - 2024 - YOLOv10 Real-Time End-to-End Object Detection.pdf;/home/navid/Zotero/storage/V869F2MI/2405.html}
}

@misc{wanHighFidelityPluralisticImage2021,
  title = {High-{{Fidelity Pluralistic Image Completion}} with {{Transformers}}},
  author = {Wan, Ziyu and Zhang, Jingbo and Chen, Dongdong and Liao, Jing},
  year = {2021},
  month = mar,
  number = {arXiv:2103.14031},
  eprint = {2103.14031},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.14031},
  urldate = {2025-01-25},
  abstract = {Image completion has made tremendous progress with convolutional neural networks (CNNs), because of their powerful texture modeling capacity. However, due to some inherent properties (e.g., local inductive prior, spatial-invariant kernels), CNNs do not perform well in understanding global structures or naturally support pluralistic completion. Recently, transformers demonstrate their power in modeling the long-term relationship and generating diverse results, but their computation complexity is quadratic to input length, thus hampering the application in processing high-resolution images. This paper brings the best of both worlds to pluralistic image completion: appearance prior reconstruction with transformer and texture replenishment with CNN. The former transformer recovers pluralistic coherent structures together with some coarse textures, while the latter CNN enhances the local texture details of coarse priors guided by the high-resolution masked images. The proposed method vastly outperforms state-of-the-art methods in terms of three aspects: 1) large performance boost on image fidelity even compared to deterministic completion methods; 2) better diversity and higher fidelity for pluralistic completion; 3) exceptional generalization ability on large masks and generic dataset, like ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/home/navid/Zotero/storage/UEVFXMRZ/Wan et al. - 2021 - High-Fidelity Pluralistic Image Completion with Transformers.pdf;/home/navid/Zotero/storage/ZBVVVDV6/2103.html}
}

@misc{wanOldPhotoRestoration2020,
  title = {Old {{Photo Restoration}} via {{Deep Latent Space Translation}}},
  author = {Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},
  year = {2020},
  month = sep,
  number = {arXiv:2009.07047},
  eprint = {2009.07047},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.07047},
  urldate = {2025-01-15},
  abstract = {We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Specifically, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with apartial nonlocal block targeting to the structured defects, such as scratches and dust spots, and a local branch targeting to the unstructured defects, such as noises and blurriness. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. Furthermore, we apply another face refinement network to recover fine details of faces in the old photos, thus ultimately generating photos with enhanced perceptual quality. With comprehensive experiments, the proposed pipeline demonstrates superior performance over state-of-the-art methods as well as existing commercial tools in terms of visual quality for old photos restoration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/home/navid/Zotero/storage/M9TB6M3M/Wan et al. - 2020 - Old Photo Restoration via Deep Latent Space Translation.pdf;/home/navid/Zotero/storage/9VP93C4Q/2009.html}
}

@misc{weiDeepRetinexDecomposition2018,
  title = {Deep {{Retinex Decomposition}} for {{Low-Light Enhancement}}},
  author = {Wei, Chen and Wang, Wenjing and Yang, Wenhan and Liu, Jiaying},
  year = {2018},
  month = aug,
  number = {arXiv:1808.04560},
  eprint = {1808.04560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1808.04560},
  urldate = {2025-01-15},
  abstract = {Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deep Retinex-Net learned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/DWFAXWTW/Wei et al. - 2018 - Deep Retinex Decomposition for Low-Light Enhancement.pdf;/home/navid/Zotero/storage/K4JL8U2D/1808.html}
}

@misc{wuDeepGenerativeModel2020,
  title = {Deep {{Generative Model}} for {{Image Inpainting}} with {{Local Binary Pattern Learning}} and {{Spatial Attention}}},
  author = {Wu, Haiwei and Zhou, Jiantao and Li, Yuanman},
  year = {2020},
  month = sep,
  number = {arXiv:2009.01031},
  eprint = {2009.01031},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.01031},
  urldate = {2025-01-24},
  abstract = {Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting. The DL-based image inpainting approaches can produce visually plausible results, but often generate various unpleasant artifacts, especially in the boundary and highly textured regions. To tackle this challenge, in this work, we propose a new end-to-end, two-stage (coarse-to-fine) generative model through combining a local binary pattern (LBP) learning network with an actual inpainting network. Specifically, the first LBP learning network using U-Net architecture is designed to accurately predict the structural information of the missing region, which subsequently guides the second image inpainting network for better filling the missing pixels. Furthermore, an improved spatial attention mechanism is integrated in the image inpainting network, by considering the consistency not only between the known region with the generated one, but also within the generated region itself. Extensive experiments on public datasets including CelebA-HQ, Places and Paris StreetView demonstrate that our model generates better inpainting results than the state-of-the-art competing algorithms, both quantitatively and qualitatively. The source code and trained models will be made available at https://github.com/HighwayWu/ImageInpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/35S6EX3H/Wu et al. - 2020 - Deep Generative Model for Image Inpainting with Local Binary Pattern Learning and Spatial Attention.pdf;/home/navid/Zotero/storage/EY6WYIL8/2009.html}
}

@misc{wuLanguageDrivenVideoInpainting2024,
  title = {Towards {{Language-Driven Video Inpainting}} via {{Multimodal Large Language Models}}},
  author = {Wu, Jianzong and Li, Xiangtai and Si, Chenyang and Zhou, Shangchen and Yang, Jingkang and Zhang, Jiangning and Li, Yining and Chen, Kai and Tong, Yunhai and Liu, Ziwei and Loy, Chen Change},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10226},
  eprint = {2401.10226},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10226},
  urldate = {2025-03-07},
  abstract = {We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/4K49J7UN/Wu et al. - 2024 - Towards Language-Driven Video Inpainting via Multimodal Large Language Models.pdf;/home/navid/Zotero/storage/L5GG2IQ2/2401.html}
}

@misc{wuLPFFPortraitDataset2023,
  title = {{{LPFF}}: {{A Portrait Dataset}} for {{Face Generators Across Large Poses}}},
  shorttitle = {{{LPFF}}},
  author = {Wu, Yiqian and Zhang, Jing and Fu, Hongbo and Jin, Xiaogang},
  year = {2023},
  month = mar,
  number = {arXiv:2303.14407},
  eprint = {2303.14407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.14407},
  urldate = {2025-02-18},
  abstract = {The creation of 2D realistic facial images and 3D face shapes using generative networks has been a hot topic in recent years. Existing face generators exhibit exceptional performance on faces in small to medium poses (with respect to frontal faces) but struggle to produce realistic results for large poses. The distorted rendering results on large poses in 3D-aware generators further show that the generated 3D face shapes are far from the distribution of 3D faces in reality. We find that the above issues are caused by the training dataset's pose imbalance. In this paper, we present LPFF, a large-pose Flickr face dataset comprised of 19,590 high-quality real large-pose portrait images. We utilize our dataset to train a 2D face generator that can process large-pose face images, as well as a 3D-aware generator that can generate realistic human face geometry. To better validate our pose-conditional 3D-aware generators, we develop a new FID measure to evaluate the 3D-level performance. Through this novel FID measure and other experiments, we show that LPFF can help 2D face generators extend their latent space and better manipulate the large-pose data, and help 3D-aware face generators achieve better view consistency and more realistic 3D reconstruction results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/HY5IN3AG/Wu et al. - 2023 - LPFF A Portrait Dataset for Face Generators Across Large Poses.pdf;/home/navid/Zotero/storage/N7E3PR7I/2303.html}
}

@misc{xuDeepFlowGuidedVideo2019,
  title = {Deep {{Flow-Guided Video Inpainting}}},
  author = {Xu, Rui and Li, Xiaoxiao and Zhou, Bolei and Loy, Chen Change},
  year = {2019},
  month = may,
  number = {arXiv:1905.02884},
  eprint = {1905.02884},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.02884},
  urldate = {2025-03-06},
  abstract = {Video inpainting, which aims at filling in missing regions of a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network. Then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/653JT5QD/Xu et al. - 2019 - Deep Flow-Guided Video Inpainting.pdf;/home/navid/Zotero/storage/AZQHZ6TA/1905.html}
}

@article{yaghmaeeImprovingImageInpainting2020,
  title = {Improving Image Inpainting Quality by a New {{SVD-based}} Decomposition},
  author = {Yaghmaee, Farzin and Peyvandi, Kimia},
  year = {2020},
  month = may,
  journal = {Multimedia Tools and Applications},
  volume = {79},
  number = {19},
  pages = {13795--13809},
  issn = {1573-7721},
  doi = {10.1007/s11042-020-08650-x},
  urldate = {2025-01-25},
  abstract = {In this paper, we present a new algorithm for image inpainting using structure and texture information. Our image decomposition to texture and structure is accomplished by the SVD method in the primary step, and then an algorithm for texture inpainting is applied. At the next level, edge detection is used in target region related to inpainted texture component. The detected edges demonstrate border of different textures in the target region, and the boundary pixels are ignored from mask temporarily. The other target pixels should be primarily inpainted, and then border pixels would be filled subsequently. Experimental results of this algorithm show better consistency in comparison with state of the art methods.},
  langid = {english},
  keywords = {Image decomposition,Image inpainting,Structure,SVD,Texture}
}

@misc{yangHunyuan3D10Unified2025,
  title = {{{Hunyuan3D}} 1.0: {{A Unified Framework}} for {{Text-to-3D}} and {{Image-to-3D Generation}}},
  shorttitle = {{{Hunyuan3D}} 1.0},
  author = {Yang, Xianghui and Shi, Huiwen and Zhang, Bowen and Yang, Fan and Wang, Jiacheng and Zhao, Hongxu and Liu, Xinhai and Wang, Xinzhou and Lin, Qingxiang and Yu, Jiaao and Wang, Lifu and Xu, Jing and He, Zebin and Chen, Zhuo and Liu, Sicong and Wu, Junta and Lian, Yihang and Yang, Shaoxiong and Liu, Yuhong and Yang, Yong and Wang, Di and Jiang, Jie and Guo, Chunchao},
  year = {2025},
  month = jan,
  number = {arXiv:2411.02293},
  eprint = {2411.02293},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02293},
  urldate = {2025-01-28},
  abstract = {While 3D generative models have greatly improved artists' workflows, the existing diffusion models for 3D generation suffer from slow generation and poor generalization. To address this issue, we propose a two-stage approach named Hunyuan3D 1.0 including a lite version and a standard version, that both support text- and image-conditioned generation. In the first stage, we employ a multi-view diffusion model that efficiently generates multi-view RGB in approximately 4 seconds. These multi-view images capture rich details of the 3D asset from different viewpoints, relaxing the tasks from single-view to multi-view reconstruction. In the second stage, we introduce a feed-forward reconstruction model that rapidly and faithfully reconstructs the 3D asset given the generated multi-view images in approximately 7 seconds. The reconstruction network learns to handle noises and in-consistency introduced by the multi-view diffusion and leverages the available information from the condition image to efficiently recover the 3D structure. Our framework involves the text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to support both text- and image-conditioned 3D generation. Our standard version has 3x more parameters than our lite and other existing model. Our Hunyuan3D 1.0 achieves an impressive balance between speed and quality, significantly reducing generation time while maintaining the quality and diversity of the produced assets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/KSHPQ7CH/Yang et al. - 2025 - Hunyuan3D 1.0 A Unified Framework for Text-to-3D and Image-to-3D Generation.pdf;/home/navid/Zotero/storage/YDNWKQ4Y/2411.html}
}

@misc{yiDiffRetinexRethinkingLowlight2023,
  title = {Diff-{{Retinex}}: {{Rethinking Low-light Image Enhancement}} with {{A Generative Diffusion Model}}},
  shorttitle = {Diff-{{Retinex}}},
  author = {Yi, Xunpeng and Xu, Han and Zhang, Hao and Tang, Linfeng and Ma, Jiayi},
  year = {2023},
  month = aug,
  number = {arXiv:2308.13164},
  eprint = {2308.13164},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.13164},
  urldate = {2025-02-18},
  abstract = {In this paper, we rethink the low-light image enhancement task and propose a physically explainable and generative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Furthermore, we hope to supplement and even deduce the information missing in the low-light image through the generative network. Therefore, Diff-Retinex formulates the low-light image enhancement problem into Retinex decomposition and conditional image generation. In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Transformer decomposition network (TDN) to decompose the image into illumination and reflectance maps. Then, we design multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution and solve the various degradations in these components respectively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light subtle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantitatively demonstrate the effectiveness, superiority, and generalization of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/navid/Zotero/storage/VM5METVK/Yi et al. - 2023 - Diff-Retinex Rethinking Low-light Image Enhancement with A Generative Diffusion Model.pdf;/home/navid/Zotero/storage/J2MD9EXZ/2308.html}
}

@misc{yuDeficiencyAwareMaskedTransformer2023,
  title = {Deficiency-{{Aware Masked Transformer}} for {{Video Inpainting}}},
  author = {Yu, Yongsheng and Fan, Heng and Zhang, Libo},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08629},
  eprint = {2307.08629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.08629},
  urldate = {2025-03-06},
  abstract = {Recent video inpainting methods have made remarkable progress by utilizing explicit guidance, such as optical flow, to propagate cross-frame pixels. However, there are cases where cross-frame recurrence of the masked video is not available, resulting in a deficiency. In such situation, instead of borrowing pixels from other frames, the focus of the model shifts towards addressing the inverse problem. In this paper, we introduce a dual-modality-compatible inpainting framework called Deficiency-aware Masked Transformer (DMT), which offers three key advantages. Firstly, we pretrain a image inpainting model DMT\_img serve as a prior for distilling the video model DMT\_vid, thereby benefiting the hallucination of deficiency cases. Secondly, the self-attention module selectively incorporates spatiotemporal tokens to accelerate inference and remove noise signals. Thirdly, a simple yet effective Receptive Field Contextualizer is integrated into DMT, further improving performance. Extensive experiments conducted on YouTube-VOS and DAVIS datasets demonstrate that DMT\_vid significantly outperforms previous solutions. The code and video demonstrations can be found at github.com/yeates/DMT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/DXEPBS7P/Yu et al. - 2023 - Deficiency-Aware Masked Transformer for Video Inpainting.pdf;/home/navid/Zotero/storage/W9BVMS3W/2307.html}
}

@misc{yuDiverseImageInpainting2021,
  title = {Diverse {{Image Inpainting}} with {{Bidirectional}} and {{Autoregressive Transformers}}},
  author = {Yu, Yingchen and Zhan, Fangneng and Wu, Rongliang and Pan, Jianxiong and Cui, Kaiwen and Lu, Shijian and Ma, Feiying and Xie, Xuansong and Miao, Chunyan},
  year = {2021},
  month = jun,
  number = {arXiv:2104.12335},
  eprint = {2104.12335},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.12335},
  urldate = {2025-01-25},
  abstract = {Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/LQLDM7N3/Yu et al. - 2021 - Diverse Image Inpainting with Bidirectional and Autoregressive Transformers.pdf;/home/navid/Zotero/storage/FVNZ9NAZ/2104.html}
}

@misc{yuFreeFormImageInpainting2019,
  title = {Free-{{Form Image Inpainting}} with {{Gated Convolution}}},
  author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas},
  year = {2019},
  month = oct,
  number = {arXiv:1806.03589},
  eprint = {1806.03589},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.03589},
  urldate = {2025-01-24},
  abstract = {We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative\_inpainting},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/navid/Zotero/storage/YVTF94XY/Yu et al. - 2019 - Free-Form Image Inpainting with Gated Convolution.pdf;/home/navid/Zotero/storage/WFQ4KNGI/1806.html}
}

@inproceedings{yuFrequencyAwareSpatiotemporalTransformers2021,
  title = {Frequency-{{Aware Spatiotemporal Transformers}} for {{Video Inpainting Detection}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Yu, Bingyao and Li, Wanhua and Li, Xiu and Lu, Jiwen and Zhou, Jie},
  year = {2021},
  month = oct,
  pages = {8168--8177},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00808},
  urldate = {2025-03-07},
  abstract = {In this paper, we propose a Frequency-Aware Spatiotemporal Transformer (FAST) for video inpainting detection, which aims to simultaneously mine the traces of video in-painting from spatial, temporal, and frequency domains. Unlike existing deep video inpainting detection methods that usually rely on hand-designed attention modules and memory mechanism, our proposed FAST have innate global self-attention mechanisms to capture the long-range relations. While existing video inpainting methods usually exploit the spatial and temporal connections in a video, our method employs a spatiotemporal transformer framework to detect the spatial connections between patches and temporal dependency between frames. As the inpainted videos usually lack high frequency details, our proposed FAST synchronously exploits the frequency domain information with a specifically designed decoder. Extensive experimental results demonstrate that our approach achieves very competitive performance and generalizes well.},
  keywords = {Computer vision,Decoding,Frequency synchronization,Frequency-domain analysis,High frequency,Image and video manipulation detection and integrity methods,Spatiotemporal phenomena,Transformers,Video analysis and understanding},
  file = {/home/navid/Zotero/storage/B5V4QXMD/Yu et al. - 2021 - Frequency-Aware Spatiotemporal Transformers for Video Inpainting Detection.pdf;/home/navid/Zotero/storage/PLLHBYII/9710661.html}
}

@misc{yuInpaintAnythingSegment2023,
  title = {Inpaint {{Anything}}: {{Segment Anything Meets Image Inpainting}}},
  shorttitle = {Inpaint {{Anything}}},
  author = {Yu, Tao and Feng, Runseng and Feng, Ruoyu and Liu, Jinming and Jin, Xin and Zeng, Wenjun and Chen, Zhibo},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06790},
  eprint = {2304.06790},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06790},
  urldate = {2025-03-07},
  abstract = {Modern image inpainting systems, despite the significant progress, often struggle with mask selection and holes filling. Based on Segment-Anything Model (SAM), we make the first attempt to the mask-free image inpainting and propose a new paradigm of ``clicking and filling'', which is named as Inpaint Anything (IA). The core idea behind IA is to combine the strengths of different models in order to build a very powerful and user-friendly pipeline for solving inpainting-related problems. IA supports three main features: (i) Remove Anything: users could click on an object and IA will remove it and smooth the ``hole'' with the context; (ii) Fill Anything: after certain objects removal, users could provide text-based prompts to IA, and then it will fill the hole with the corresponding generative content via driving AIGC models like Stable Diffusion; (iii) Replace Anything: with IA, users have another option to retain the click-selected object and replace the remaining background with the newly generated scenes. We are also very willing to help everyone share and promote new projects based on our Inpaint Anything (IA). Our codes are available at https://github.com/geekyutao/Inpaint-Anything.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/4QZTSTNH/Yu et al. - 2023 - Inpaint Anything Segment Anything Meets Image Inpainting.pdf;/home/navid/Zotero/storage/7VLHP6Z4/2304.html}
}

@article{zhangExploitingOpticalFlow2024,
  title = {Exploiting {{Optical Flow Guidance}} for {{Transformer-Based Video Inpainting}}},
  author = {Zhang, Kaidong and Peng, Jialun and Fu, Jingjing and Liu, Dong},
  year = {2024},
  month = jul,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {7},
  eprint = {2301.10048},
  primaryclass = {cs},
  pages = {4977--4992},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2024.3361010},
  urldate = {2025-03-06},
  abstract = {Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/T47AQPFK/Zhang et al. - 2024 - Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting.pdf;/home/navid/Zotero/storage/5AU4F2JH/2301.html}
}

@misc{zhangFasterSegmentAnything2023,
  title = {Faster {{Segment Anything}}: {{Towards Lightweight SAM}} for {{Mobile Applications}}},
  shorttitle = {Faster {{Segment Anything}}},
  author = {Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung-Ho and Lee, Seungkyu and Hong, Choong Seon},
  year = {2023},
  month = jul,
  number = {arXiv:2306.14289},
  eprint = {2306.14289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.14289},
  urldate = {2025-03-07},
  abstract = {Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot transfer performance and high versatility for numerous vision applications (like image editing with fine-grained control). Many of such applications need to be run on resource-constraint edge devices, like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance, especially when limited training sources are available. We find that this is mainly caused by the coupled optimization of the image encoder and mask decoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM. The training can be completed on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM which is more than 60 times smaller yet performs on par with the original SAM. For inference speed, With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover, we show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided at {\textbackslash}href\{https://github.com/ChaoningZhang/MobileSAM\}\{{\textbackslash}textcolor\{red\}\{MobileSAM\}\}), with a demo showing that MobileSAM can run relatively smoothly on CPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/4PAW5Y8N/Zhang et al. - 2023 - Faster Segment Anything Towards Lightweight SAM for Mobile Applications.pdf;/home/navid/Zotero/storage/67N7S87J/2306.html}
}

@misc{zhangFlowGuidedTransformerVideo2022,
  title = {Flow-{{Guided Transformer}} for {{Video Inpainting}}},
  author = {Zhang, Kaidong and Fu, Jingjing and Liu, Dong},
  year = {2022},
  month = aug,
  number = {arXiv:2208.06768},
  eprint = {2208.06768},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.06768},
  urldate = {2025-03-06},
  abstract = {We propose a flow-guided transformer, which innovatively leverage the motion discrepancy exposed by optical flows to instruct the attention retrieval in transformer for high fidelity video inpainting. More specially, we design a novel flow completion network to complete the corrupted flows by exploiting the relevant flow features in a local temporal window. With the completed flows, we propagate the content across video frames, and adopt the flow-guided transformer to synthesize the rest corrupted regions. We decouple transformers along temporal and spatial dimension, so that we can easily integrate the locally relevant completed flows to instruct spatial attention only. Furthermore, we design a flow-reweight module to precisely control the impact of completed flows on each spatial transformer. For the sake of efficiency, we introduce window partition strategy to both spatial and temporal transformers. Especially in spatial transformer, we design a dual perspective spatial MHSA, which integrates the global tokens to the window-based attention. Extensive experiments demonstrate the effectiveness of the proposed method qualitatively and quantitatively. Codes are available at https://github.com/hitachinsk/FGT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/LW8PKKSB/Zhang et al. - 2022 - Flow-Guided Transformer for Video Inpainting.pdf;/home/navid/Zotero/storage/77HNV2XC/2208.html}
}

@misc{zhangInternalLearningApproach2019,
  title = {An {{Internal Learning Approach}} to {{Video Inpainting}}},
  author = {Zhang, Haotian and Mai, Long and Xu, Ning and Wang, Zhaowen and Collomosse, John and Jin, Hailin},
  year = {2019},
  month = sep,
  number = {arXiv:1909.07957},
  eprint = {1909.07957},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.07957},
  urldate = {2025-03-06},
  abstract = {We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/R65XN8TN/Zhang et al. - 2019 - An Internal Learning Approach to Video Inpainting.pdf;/home/navid/Zotero/storage/9R3BDCZ9/1909.html}
}

@misc{zhangMobileSAMv2FasterSegment2023,
  title = {{{MobileSAMv2}}: {{Faster Segment Anything}} to {{Everything}}},
  shorttitle = {{{MobileSAMv2}}},
  author = {Zhang, Chaoning and Han, Dongshen and Zheng, Sheng and Choi, Jinwoo and Kim, Tae-Ho and Hong, Choong Seon},
  year = {2023},
  month = dec,
  number = {arXiv:2312.09579},
  eprint = {2312.09579},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09579},
  urldate = {2025-03-07},
  abstract = {Segment anything model (SAM) addresses two practical yet challenging segmentation tasks: {\textbackslash}textbf\{segment anything (SegAny)\}, which utilizes a certain point to predict the mask for a single object of interest, and {\textbackslash}textbf\{segment everything (SegEvery)\}, which predicts the masks for all objects on the image. What makes SegAny slow for SAM is its heavyweight image encoder, which has been addressed by MobileSAM via decoupled knowledge distillation. The efficiency bottleneck of SegEvery with SAM, however, lies in its mask decoder because it needs to first generate numerous masks with redundant grid-search prompts and then perform filtering to obtain the final valid masks. We propose to improve its efficiency by directly generating the final masks with only valid prompts, which can be obtained through object discovery. Our proposed approach not only helps reduce the total time on the mask decoder by at least 16 times but also achieves superior performance. Specifically, our approach yields an average performance boost of 3.6{\textbackslash}\% (42.5{\textbackslash}\% {\textbackslash}textit\{v.s.\} 38.9{\textbackslash}\%) for zero-shot object proposal on the LVIS dataset with the mask AR@\$K\$ metric. Qualitative results show that our approach generates fine-grained masks while avoiding over-segmenting things. This project targeting faster SegEvery than the original SAM is termed MobileSAMv2 to differentiate from MobileSAM which targets faster SegAny. Moreover, we demonstrate that our new prompt sampling is also compatible with the distilled image encoders in MobileSAM, contributing to a unified framework for efficient SegAny and SegEvery. The code is available at the same link as MobileSAM Project {\textbackslash}href\{https://github.com/ChaoningZhang/MobileSAM\}\{{\textbackslash}textcolor\{red\}\{https://github.com/ChaoningZhang/MobileSAM\}\}. {\textbackslash}end\{abstract\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/TKNDHJSF/Zhang et al. - 2023 - MobileSAMv2 Faster Segment Anything to Everything.pdf;/home/navid/Zotero/storage/HX3R2UDQ/2312.html}
}

@misc{zhaoHunyuan3D20Scaling2025,
  title = {{{Hunyuan3D}} 2.0: {{Scaling Diffusion Models}} for {{High Resolution Textured 3D Assets Generation}}},
  shorttitle = {{{Hunyuan3D}} 2.0},
  author = {Zhao, Zibo and Lai, Zeqiang and Lin, Qingxiang and Zhao, Yunfei and Liu, Haolin and Yang, Shuhui and Feng, Yifei and Yang, Mingxin and Zhang, Sheng and Yang, Xianghui and Shi, Huiwen and Liu, Sicong and Wu, Junta and Lian, Yihang and Yang, Fan and Tang, Ruining and He, Zebin and Wang, Xinzhou and Liu, Jian and Zuo, Xuhui and Chen, Zhuo and Lei, Biwen and Weng, Haohan and Xu, Jing and Zhu, Yiling and Liu, Xinhai and Xu, Lixin and Hu, Changrong and Huang, Tianyu and Wang, Lifu and Zhang, Jihong and Chen, Meng and Dong, Liang and Jia, Yiwen and Cai, Yulin and Yu, Jiaao and Tang, Yixuan and Zhang, Hao and Ye, Zheng and He, Peng and Wu, Runzhou and Zhang, Chao and Tan, Yonghao and Xiao, Jie and Tao, Yangyu and Zhu, Jianchen and Xue, Jinbao and Liu, Kai and Zhao, Chongqing and Wu, Xinming and Hu, Zhichao and Qin, Lei and Peng, Jianbing and Li, Zhan and Chen, Minghui and Zhang, Xipeng and Niu, Lin and Wang, Paige and Wang, Yingkai and Kuang, Haozhao and Fan, Zhongyi and Zheng, Xu and Zhuang, Weihao and He, YingPing and Liu, Tian and Yang, Yong and Wang, Di and Liu, Yuhong and Jiang, Jie and Huang, Jingwei and Guo, Chunchao},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12202},
  eprint = {2501.12202},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12202},
  urldate = {2025-01-28},
  abstract = {We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/QCKPT9XK/Zhao et al. - 2025 - Hunyuan3D 2.0 Scaling Diffusion Models for High Resolution Textured 3D Assets Generation.pdf;/home/navid/Zotero/storage/LQWDZBMD/2501.html}
}

@misc{zhaoLossFunctionsNeural2018,
  title = {Loss {{Functions}} for {{Neural Networks}} for {{Image Processing}}},
  author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
  year = {2018},
  month = apr,
  number = {arXiv:1511.08861},
  eprint = {1511.08861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.08861},
  urldate = {2025-01-29},
  abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is L2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/EMRVW6M5/Zhao et al. - 2018 - Loss Functions for Neural Networks for Image Processing.pdf;/home/navid/Zotero/storage/JPLK9UBQ/1511.html}
}

@misc{zhengBridgingGlobalContext2021,
  title = {Bridging {{Global Context Interactions}} for {{High-Fidelity Image Completion}}},
  author = {Zheng, Chuanxia and Cham, Tat-Jen and Cai, Jianfei and Phung, Dinh},
  year = {2021},
  month = nov,
  number = {arXiv:2104.00845},
  eprint = {2104.00845},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.00845},
  urldate = {2025-01-25},
  abstract = {Bridging global context interactions correctly is important for high-fidelity image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose to treat image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence in the encoder. Crucially, we employ a restrictive CNN with small and non-overlapping RF for weighted token representation, which allows the transformer to explicitly model the long-range visible context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. To improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related high-frequency features. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/UFR96AXN/Zheng et al. - 2021 - Bridging Global Context Interactions for High-Fidelity Image Completion.pdf;/home/navid/Zotero/storage/AV4XI7AS/2104.html}
}

@misc{zhouPlacesImageDatabase2016,
  title = {Places: {{An Image Database}} for {{Deep Scene Understanding}}},
  shorttitle = {Places},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Torralba, Antonio and Oliva, Aude},
  year = {2016},
  month = oct,
  number = {arXiv:1610.02055},
  eprint = {1610.02055},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.02055},
  urldate = {2025-01-24},
  abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/ZG58PSHF/Zhou et al. - 2016 - Places An Image Database for Deep Scene Understanding.pdf;/home/navid/Zotero/storage/8ZJ8QSCT/1610.html}
}

@misc{zhouProPainterImprovingPropagation2023,
  title = {{{ProPainter}}: {{Improving Propagation}} and {{Transformer}} for {{Video Inpainting}}},
  shorttitle = {{{ProPainter}}},
  author = {Zhou, Shangchen and Li, Chongyi and Chan, Kelvin C. K. and Loy, Chen Change},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03897},
  eprint = {2309.03897},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.03897},
  urldate = {2025-03-06},
  abstract = {Flow-based propagation and spatiotemporal Transformer are two mainstream mechanisms in video inpainting (VI). Despite the effectiveness of these components, they still suffer from some limitations that affect their performance. Previous propagation-based approaches are performed separately either in the image or feature domain. Global image propagation isolated from learning may cause spatial misalignment due to inaccurate optical flow. Moreover, memory or computational constraints limit the temporal range of feature propagation and video Transformer, preventing exploration of correspondence information from distant frames. To address these issues, we propose an improved framework, called ProPainter, which involves enhanced ProPagation and an efficient Transformer. Specifically, we introduce dual-domain propagation that combines the advantages of image and feature warping, exploiting global correspondences reliably. We also propose a mask-guided sparse video Transformer, which achieves high efficiency by discarding unnecessary and redundant tokens. With these components, ProPainter outperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining appealing efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/LUWF7XED/Zhou et al. - 2023 - ProPainter Improving Propagation and Transformer for Video Inpainting.pdf;/home/navid/Zotero/storage/4YBMT4XY/2309.html;/home/navid/Zotero/storage/A4PYILU2/2309.html;/home/navid/Zotero/storage/QRZD375W/2309.html}
}

@misc{zhuCelebVHQLargeScaleVideo2022,
  title = {{{CelebV-HQ}}: {{A Large-Scale Video Facial Attributes Dataset}}},
  shorttitle = {{{CelebV-HQ}}},
  author = {Zhu, Hao and Wu, Wayne and Zhu, Wentao and Jiang, Liming and Tang, Siwei and Zhang, Li and Liu, Ziwei and Loy, Chen Change},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12393},
  eprint = {2207.12393},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.12393},
  urldate = {2025-01-25},
  abstract = {Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,666 video clips with the resolution of 512x512 at least, involving 15,653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ. Besides, its versatility and potential are validated on two representative tasks, i.e., unconditional video generation and video facial attribute editing. Furthermore, we envision the future potential of CelebV-HQ, as well as the new opportunities and challenges it would bring to related research directions. Data, code, and models are publicly available. Project page: https://celebv-hq.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/navid/Zotero/storage/TBNG25M8/Zhu et al. - 2022 - CelebV-HQ A Large-Scale Video Facial Attributes Dataset.pdf;/home/navid/Zotero/storage/JR2NA6KB/2207.html}
}
